\documentclass[a4paper, oneside]{discothesis}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{bbm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{mathtools} 
\usepackage{csvsimple}
\usepackage{multirow}
\usepackage{amsmath}
\setcounter{secnumdepth}{3}
\usepackage{biblatex}

\addbibresource{references.bib}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DOCUMENT METADATA

\thesistype{Master Thesis\\ MSc Quantitative Finance ETH/UZH} % Master's Thesis, Bachelor's Thesis, Semester Thesis, Group Project
\title{Comparison of Statistical and Machine Learning Methods in Modelling Time-Varying Volatility 
}

\author{Georgios Avgoustinos}
\email{georgios.avgoustinos@uzh.ch}

\institute{Department of Banking and Finance\\
University of Zurich}

% Optionally, you can put in your own logo here
%\logo{\includegraphics[width=0.2\columnwidth]{figures/disco_logo_faded}}

\supervisors{Prof. Dr. Erich Walter Farkas\\
 Zaid Siddiqi (Zanders)}

% Optionally, keywords and categories of the work can be shown (on the Abstract page)
%\keywords{Keywords go here.}
%\categories{ACM categories go here.}

\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\frontmatter % do not remove this line
\maketitle

\cleardoublepage

\begin{acknowledgements}
	I thank Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.
\end{acknowledgements}


\begin{abstract}
    The abstract should be short, stating what you did and what the most important result is.
	Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.
\end{abstract}

\tableofcontents

\mainmatter % do not remove this line

% Start writing here
\chapter{Introduction}

\chapter{Literature Overview of Statistical Models in Time Varying Volatility}

\section{One Dimensional Models}
\todo[]{More details in the specification of Garch, Egarch, GJR. Some remarks and probably unconditional variance or forecasting close form.}
In the specific section we focus on modelling conditional volatility in one dimensional time series of returns. 


We will introduce three important statistical models widely used for volatility modelling. After that, we will describe the problem formulation with the methodology used and the different models used. Finally, we will provide a detailed comparison of the studied models.


The main objective of the section is to introduce to the reader three milestone statistical models that will be later used as points of reference. 

To put things formally, we work in a usual probability triplet $\left( \Omega, \mathcal{F}, \mathbb{P}\right)$ equipped with the filtration $\mathbb{F} = \left(\mathcal{F}_t\right)_{t = 1, 2, \dots}$ which is generated by the available information of the market at each time step. We denote the asset's returns with the $\mathbb{F}$-adapted process  $\left(x_t\right)_{t = 1, 2, \dots}$ and our main assumption is that for $t = 1,2,\dots$ 

\begin{equation}\label{eq:1d-ass}
x_t \sim \mathcal{N}\left(\mu, \sigma_t\right)
\end{equation}

where $\sigma_t$ is a $\mathbb{F}$-predictable stochastic process denoting the conditional volatility at each time step.

Notice that we choose to consider the mean of the returns constant over time as we mainly focus on modelling the conditional volatility. 
Moreover, we decide to continue with the assumption that conditional returns are normally distributed - even though Students-t extensions exist in the literature - just for the shake of comparison with the challengers models - see later -  and without loss of generality.

\subsection{GARCH}

Generalized Autoregressive Conditional Heteroskedastic model was introduced by the author \cite{garch} and can be considered to be the starting point of volatility modelling.

\begin{definition}[GARCH $\left(p, q\right)$]\label{thm:garch_def}

$\left(x_t\right)_{t\in \mathbb{N}}$ follows a normal GARCH $\left(p, q\right)$ process with zero mean, if $\forall t \in \mathbb{N}$ holds that

\begin{gather*}\label{eq:1}
x_t = \sigma_t \epsilon_t \\
\sigma^2_t = \omega + \sum_{i=1}^p\alpha_i x_{t-i}^2 + \sum_{j=1}^q \beta_j 
\sigma_{t-j}^2  \\
\epsilon_t \sim \mathcal{N}\left(0,1\right)
\end{gather*}

\end{definition}
where $\omega > 0, \alpha_i \geq 0, i = 1,\dots,p$ and $\beta_j \geq 0, j = 1,\dots,q$

Even though, the model is simple enough, it is able to capture important features of financial returns data such as the volatility clustering. On the other hand due to the symmetry in the formula the \textit{leverage effect} cannot be captured and both upward and downward movements of the market seem to affect the volatility in the same way.

The parameters $p$ and $q$ are noting the time lag of the returns and volatility respectively.

\subsection{EGARCH}

The author \cite{egarch} introduced the exponential GARCH model as a further extension of \ref{thm:garch_def}  to deal with the leverage feature.

\begin{definition}[EGARCH $\left(p, q\right)$]\label{thm:egarch_def} $\left(x_t\right)_{t\in \mathbb{N}}$ follows a normal EGARCH $\left(p, q\right)$ process with zero mean, if $\forall t \in \mathbb{N}$ holds that

\begin{gather*}\label{eq:}
x_t = \sigma_t \epsilon_t \\
\log\sigma^2_t = \omega + \sum_{i=1}^p\alpha_i \left(|\epsilon_{t-i}|+\gamma_i\epsilon_{t-i}\right) + \sum_{j=1}^q \beta_j \log \sigma_{t-j}^2  \\
\epsilon_t \sim \mathcal{N}\left(0,1\right)
\end{gather*}

\end{definition}
where the $\gamma_i$ parameters signifies the asymmetric effects from $x_{t-i}$. 

\subsection{GJR}

In continuation with the aim of capturing better the leverage effect, the authors \cite{gjr} suggested the Glosten-Jagannathan-Runkle GARCH model. 

\begin{definition}[GJR]\label{thm:gjr} 
$\left(x_t\right)_{t\in \mathbb{N}}$ follows a normal GJR process with zero mean, if $\forall t \in \mathbb{N}$ holds that

\begin{gather*}\label{eq:}
x_t = \sigma_t \epsilon_t \\
\sigma^2_t = \omega + \left(\alpha+\gamma \mathbbm{1}_{x_{t-1}<0}\right)x_{t-1}^2+\beta \sigma_{t-1}^2 \\
\epsilon_t \sim \mathcal{N}\left(0,1\right)
\end{gather*}
\end{definition}


\subsection{Model Estimation}

For the parameter estimation of the mentioned models the method which is used is \textit{Maximum Likelihood Estimation (MLE)}.

More precisely, we denote the returns vector by $\mathbf{X}$ and without loss of generality we may consider time lag of one step. 
\begin{equation}
    f_{\bm{X}}(x_1, \dots, x_n) = f_{X_1}(x_1)\prod_{t=1}^{n-1}f_{X_{t+1}|X_t}(x_{t+1}|x_t)
\end{equation}


Denoting by $\bm{\theta}$ the model parameters, one can write the conditional variance in a general form of $\sigma_t^2 = g(\bm{\theta}, x_{t-1})$ for a measurable function $g$ which is determined from the specific model. 

Switching to the log-prices, one has
\begin{equation}
    L\left(\bm{\theta}; \bm{X}\right) = \log f_{\bm{X}}(x_1, \dots, x_n) = \log f_{X_1}(x_1) + \sum_{t=1}^{n-1} \log  f_{X_{t+1}|X_t}(x_{t+1}|x_t) 
\end{equation}

According to the normal zero-mean assumption of the returns,

\begin{equation}
\begin{split}
      L\left(\bm{\theta}; \bm{X}\right) = -\frac{1}{2}\sum_{t=1}^n \log(2\pi)+\log(\sigma_t^2)+\frac{x_t^2}{\sigma_t^2}=\\-\frac{1}{2}\left(n\log(2\pi)+\sum_{t=1}^n \log(g(\bm{\theta}, x_{t-1}))+\frac{x_t^2}{g(\bm{\theta}, x_{t-1})}\right)
\end{split}
\end{equation}

thereby, the optimal parameters can be viewed as solutions to the optimization problem:

\begin{equation}
    \bm{\theta}^* = \underset{\bm{\theta}}{\mathrm{argmin }} -L\left(\bm{\theta}; \bm{X}\right)
\end{equation}

The optimal parameters in general are a local maximum and solve the equations

\begin{equation}\label{eq:scores}
    \frac{\partial}{\partial\bm{\theta}}L\left(\bm{\theta}; \bm{X}\right) = 0
\end{equation}

where the left-hand side is known as the \textit{score vector}.  

The equation \ref{eq:scores} is solved numerically with Newton-Raphson methods. The one most typically used is the BHHH \cite{bhhh} method.

\section{Multidimensional Models}

\chapter{Machine Learning Models in Scope}
Machine Learning (ML) algorithms are widely used in a large variety of tasks nowadays when the computational power of modern machines has dramatically increased. 

These tasks include problems such as image classification \cite{imageNet} or sequential learning  \cite{https://doi.org/10.48550/arxiv.1706.03762} among many others.

We will introduce the reader to some of the ML algorithms that can be used for function approximation and later on provide a detailed framework for applying this basic ideas in the concept of time-varying volatility estimation.  

\section{Neural Networks}
\todo[]{Are we adding figures with the architectures of the networks???}
\subsection{Feed-forward Neural Networks}

Feed-forward neural networks (FNN) is a popular method used in ML and data science for both regression and classification tasks. Intuitively they can be understood as high dimensional non linear regression functions. They have their roots back in the 1940s and the main mathematical guarantee is derived from the Universal Approximation Theorem \cite{Cybenko1989ApproximationBS}. 

\theorem (Universal Approximation Theorem - G. Cybenko) Let $\sigma$ be a sigmoidal function 
\begin{equation}
\sigma(x) = \left\{ \begin{array}{@{}ll@{}}
    1, &  x\rightarrow+\infty  \\
    0, & x\rightarrow-\infty
\end{array}\right.
\end{equation}
then finite sums of the form 
\begin{equation}
g(x) = \sum_{j=1}^{N} w_j^2\sigma((w^1_j)^\intercal x+b_j)
\end{equation}
are dense in $C(I_n)$ with respect to the supremum norm. Where $x\in \mathbb{R}^n$, $w_j^1\in  \mathbb{R}^n$, $w_j^2\in \mathbb{R}$, $b_j^1\in \mathbb{R}$. $I_n$ is the $n$-dimensional unit cube $[0,1]^n$ and $C(I_n)$ is noted for the space of continuous functions on $I_n$.

In other words, given any $f\in C(I_n)$ and $\epsilon >0$, there is a sum, $g(x)$, of the above form, for which
\begin{equation}
|g(x)-f(x)|<\epsilon
\end{equation}
for all $x\in I_n$.

In what follows, we will give a definition of a FNN layer and general architecture.

\definition (FNN Layer) \label{def:nn_layer}A neural network layer $\bm{z^m}$ with activation function $\phi:\mathbb{R}\mapsto\mathbb{R}$ is a function of the form
\begin{equation}
\mathbb{R}^{q_{m-1}}\ni \bm{z} \longmapsto \bm{z^m}(\bm{z}) \in \mathbb{R}^{q_{m}}
\end{equation}
Each one of the elements of the layer is called a \textit{neuron} and is of the form
\begin{equation}
\bm{z^m}(\bm{z})_j = \phi \left(w_{j,0}^m+\sum_{i=1}^{q_{m-1}}w_{j,i}^m\bm{z}_i\right)\eqqcolon\phi\left(\right(\bm{w^m}\left) ^\intercal\bm{z}\right)\text{, for } j = 1, \dots, q_m
\end{equation}
given the layer's weights $\bm{w^m}\in \mathbb{R}^{q_{m-1}+1}$. For the economy of the notation we will use $\bm{z^m}(\bm{z}) = \phi \left(\bm{W^m z} \right)$.

\definition (FNN Architecture) A Feed-forward neural network is a composition of several layers. In other words a function $\bm{z^{(m:1)}}$ of the form
\begin{equation}
\mathbb{R}^p\ni \bm{x} \longmapsto \bm{z^{(m:1)}}(\bm{x}) = \left(\bm{z^m}\circ\cdots\circ\bm{z^1}\right)(\bm{x})\in \mathbb{R}^q
\end{equation}
We call $p, q$ input and output dimension respectively and $m$ the depth of the network. If $m=1$ the network is called \textit{shallow} otherwise is called \textit{deep}.

\subsection{Recurrent Neural Networks}
Recurrent neural networks (RNN) are models used in ML mainly for sequential learning tasks such as speech recognition \cite{speechRNN} or time series prediction \cite{Hewamalage_2021}. 

We will provide the main idea of a Simple RNN and extend it to the Long Short Term Memory Networks (LSTM) networks that follow.

We start with sequential data of the form $\bm{X} = \left(\bm{x_1}, \dots, \bm{x_T}\right)^\intercal $ where $\bm{x_t}\in\mathbb{R}^{\tau}$ - $t$ can be illustrated as a specific time step -  for $t = 1, \dots, T$. Then, a Simple RNN is consisted of a single layer of the following form similar to \ref{def:nn_layer}. 

\definition(Simple RNN Layer) A simple RNN layer is a function $\bm{z^1}$ of the following form
\begin{equation}
\begin{split}
\mathbb{R}^{\tau\times q_1} \ni (\bm{x_t}, \bm{z_{t-1}}) \longmapsto \bm{z_t} = \bm{z^1}(\bm{x_t}, \bm{z_{t-1}}) \in \mathbb{R}^{q_1} \\ \text{, for } t = 1, \dots, T
\end{split}
\end{equation}

Each node of the Simple RNN layer is of the form 
\begin{equation}
\begin{split}
z^1(\bm{x_t}, \bm{z_{t-1}})_j = z_{t,1} = \phi \left(w_{j,0}^1+\sum_{i=1}^{\tau}w_{j,i}^1x_{t,i}+\sum_{k = 1}^{q_1}u_{j,k}^1 z_{t-1, k}\right)\\\eqqcolon\phi\left(\right(\bm{w^1}\left) ^\intercal\bm{x_t}+\left(\bm{u^1}\right)^\intercal \bm{z_{t-1}}\right) \text{, for } j = 1, \dots, q_m
\end{split}
\end{equation}

Similarly, for the easing of the notation we write $\bm{z^1}(\bm{x_t}, \bm{z_{t-1}}) = \phi\left( \bm{W^1}\bm{x_t} + \bm{U^1}\bm{z_{t-1}}\right)$

\subsection{Long Short Term Memory Networks}
LSTM networks are a very popular - but more complicated - types of RNNs introduced by \cite{LSTM}. Hence, the recursive functional behaviour still appears but with a different layer. 

The most important improvement is that LSTM is more computationally efficient during the backpropagation stage - see \ref{ssec:num1} - as it deals with the problem of vanishing gradients. In general, it filters the explanatory variables with activation function called "gates". 
A LSTM layer uses at least two different activation functions:
\begin{itemize}
    \item $\phi_\sigma = \frac{1}{1+e^{-x}} \in (0,1)$ (\textit{sigmoid function})
    \item $\phi_{\text{tanh}} = 2\phi_\sigma(2x)-1 \in (-1,1)$ (\textit{hyperbolic tangent function})
    \item $\phi: \mathbb{R}\mapsto\mathbb{R}$ a general prespecified activation function
\end{itemize}

\definition(LSTM Layer) For an eplanatory dataset of the form $\bm{X} = \left(\bm{x_1}, \dots, \bm{x_T}\right)^\intercal\in \mathbb{R}^{T\times \tau} $ the compact form of a forward pass in LSTM layer can be denoted with the following equations:

\begin{gather}
    \bf{f_t} = \phi_\sigma\left(\bf{W_f}\bf{x_t}+\bf{U_f}\bf{z_{t-1}+\bf{b_f}}\right) \text{  (forget gate)}\\
    \bf{i_t} = \phi_\sigma\left(\bf{W_i}\bf{x_t}+\bf{U_i}\bf{z_{t-1}+\bf{b_i}}\right) \text{  (input/update gate)}\\
    \bf{o_t} = \phi_\sigma\left(\bf{W_o}\bf{x_t}+\bf{U_o}\bf{z_{t-1}+\bf{b_o}}\right) \text{  (output gate)}\\
    \bf{\tilde{c}_t} = \phi_{\text{tanh}}\left(\bf{W_c}\bf{x_t}+\bf{U_c}\bf{z_{t-1}+\bf{b_c}}\right) \text{   (cell input activation)}\\
    \bf{c_t} = \bf{f_t}\bullet \bf{c_{t-1}}+\bf{i_t}\bullet\bf{\tilde{c}_t} \text{  (cell state vector)}\\
    \bf{z_t} = \bf{o_t}\bullet\phi(\bf{c_t}) \text{  (final output)}
\end{gather}

where $\bf{z_t}\in\mathbb{R}^{q_1}$ and $q_1$ is the output dimension.

\subsection{Optimization Methods for Neural Networks}\label{ssec:num1}

The methods used for estimating the layers parameters that best fit to a given data set are Gradient Descent based algorithms which are based in backpropagation \cite{Linnainmaa1976} among the layers of each neural network. For a more detailed view on backpropagation in neural networks the reader is referred to \cite{murphyMLProba}.
\section{Gradient Boosting Methods}

Gradient boosting algorithms are additive estimators consisted of multiple simple regression functions such us linear regressors or decision trees. For the scope of this master thesis we will present the algorithm the gradient boosting method with decision trees which will be later on used in the framework of conditional volatility estimation.

These method in general starts with a basic estimator for some target variable and recursively improves the estimator by performing aggregation based on the reduction of a specified loss function.

For a detailed description of regression trees the reader is referred to \cite{hastie01statisticallearning}, as the specific regression method will be only used as a part of the future models and not as an estimator by its own.

\subsection{Gradient Tree Boosting Algorithm}

The specific algorithm is an extension of the \textit{Functional Gradient Descent} introduced by \cite{FriedFGB}.

Suppose we have a dataset $\mathcal{D} = \left(x_i\right)_{i=1}^N$ where $x_i\in \mathbb{R}^d$ are explanatory variables labeled with the target vector $y = \left(y_1, \dots, y_N\right)^\intercal\in \mathbb{R}^N$. We are interested in finding a parametrized function $\mathbb{R}^d \ni x \mapsto \hat{f}(x; \bf{\theta})\in \mathbb{R}$ s.t. for a specified loss function $l:\mathbb{R}^2\mapsto\mathbb{R}$, the average of losses $\frac{1}{N}\sum_{i=1}^{N}l(\hat{f}(x_i; \bf{\theta}), y_i)$ is minimized.
The proposed algorithm is the following.

\begin{algorithm}
\caption{Gradient Tree Boosting}\label{alg:bg}
\begin{algorithmic}
\State \begin{enumerate} 

\item $\text{Initialize } f_0(x) = \underset{\gamma}{\mathrm{argmin }} \sum_{i=1}^N l(y_i, \gamma)$
\item For $m = 1, \dots, M$:

\begin{enumerate}
\item Compute for $i = 1, \dots, N$\[r_{i,m} = - \left[\frac{\partial l\left(y_i, f(x_i)\right)}{\partial f(x_i)}\right]_{f = f_{m-1}}\]

\item Fit a decision tree regressor with target variables $(r_{i,m})_{i=1}^N$ and explanatory variables $(x_i)_{i=1}^N$. Denote the tree's leaves by $R_{j, m}$, $j = 1, \dots, J_m$

\item For $j = 1,\dots, J_m$:
\[ \gamma_{j,m} = \underset{\gamma}{\mathrm{argmin }}\underset{x_i\in R_{j,m}}{\sum}l(y_i, f_{m-1}(x_i)+\gamma)\]
\item Update \[f_m(x) = f_{m-1}(x)+\sum_{j=1}^{J_m}\gamma_{j,m} \mathbbm{1}_{x\in R_{j,m}}\]

\end{enumerate}
\item Output \[\hat{f}(x) = f_M(x)\]

\end{enumerate} 
\end{algorithmic}
\end{algorithm}

\chapter{Estimation of Models for One-Dimensional Returns}

\section{Model Assumptions}

As mentioned already, we choose to follow a distributional assumption for the asset's returns. This is because, we are interested in further applications of the proposed models like in the fields of Portfolio Optimization, Market Risk Modelling and Asset Liability Management where volatility modelling plays a crucial role.

Therefore, we assume that the returns are conditionally normal distributed with time varying volatility.

For the shake of numerical stability and without loss of generality, we will work with 0-centered returns which implies that we can summarize the return's dynamics as follows.

\begin{assumption}\label{as:main1d}
We work on the probability space $\left(\Omega, \mathcal{F}, \mathbb{P}\right)$, denote the asset's returns by $\mathbf{r} = (r_1, \dots, r_T)^\intercal$ and augment the probability space with the filtration generated from the returns $\mathbb{F} = (\mathcal{F}_t)_{t=0}^T$ (where $\mathcal{F}_0$ is chosen to be the trivial $\sigma$ - algebra). We assume that for $t=1, \dots, T$, $r_{t}$ are conditionally distributed as follows:
\begin{equation}
\begin{split}
r_{t}|\mathcal{F}_{t-1}\sim \mathcal{N}(0, \sigma_{t})\\
\sigma_{t} = f^{\text{model}}(\mathcal{F}_{t-1}) 
\end{split}
\end{equation}

where $f^{\text{model}}$ is a well defined function defined from the relative used model.
\end{assumption}
\section{Specification of the Loss Function}

An important step for the model estimation is the specification of the loss function for minimization.

The focus on a distributional approach for our modelling leads us to select MLE methods for the estimation of the model. According to \ref{as:main1d}, from the obtained log-likelihood of the returns, we can select the following loss function for minimization $l:\mathbb{R}^T\times\mathbb{R}^T\mapsto\mathbb{R}$.
\begin{equation}\label{eq:loss1d}
    l(\mathbf{r}, \hat{\mathbf{\sigma}}) = \\\frac{1}{2}\left(n\log(2\pi)+\sum_{t=1}^n \log(\hat{\sigma}_t^2)+\frac{r_t^2}{\hat{\sigma}_t^2}\right)
\end{equation}
With $\hat{\mathbf{\sigma}}\in \mathbb{R}^T$ we denote the estimated conditional volatility.

\begin{remark}
The specific loss function in \ref{eq:loss1d} will be used for both model fitting and performance measurement reasons throughout the chapter. 
\end{remark}
\section{Data Handling \& Model Fitting}
We start with a data set containing the daily spot prices of an asset, denoted by $\mathbf{S} = (S_0, S_1, \dots, S_T)^\intercal$ and then we define the daily percentage returns as:
\begin{equation}
    \tilde{r}_{t} = 100\frac{S_{t}-S_{t-1}}{S_{t-1}} \text{,  for } t = 1, \dots, T 
\end{equation}
as mentioned we switch to the 0-centered returns:
\begin{equation}
    r_t = \tilde{r}_t - \frac{\sum_{t = 1}^{T}\tilde{r}_t}{T} \text{,  for } t = 1, \dots, T 
\end{equation}

An additional variable that will be later on used as an extra feature for the estimation is the log-realized-volatility defined as follows. 
\begin{equation}\label{eq:rv}
    \varsigma_t = \log\left(\sqrt{\frac{1}{22}\sum_{i = t-21}^t \left(r_i - \frac{\sum_{j=t-21}^t r_j}{22}\right)^2}\right)\text{,  for } t = 22, \dots, T 
\end{equation}

As a consequence, we can now construct an explanatory data set $\mathcal{D} = (\mathbf{x_t})_{t=21+\lambda}^T$ for the estimation of the conditional volatility where 
\begin{equation}
    \mathbf{x_t} = (r_{t-\lambda+1}, \dots, r_{t}, \varsigma_{t-\lambda+1},\dots, \varsigma_{t})^\intercal\in\mathbb{R}^{2\lambda}
\end{equation}
where $\lambda>1$ (see \ref{ref: lag}).

\begin{remark}
Equation \ref{eq:rv} is derived from the realized standard deviation of the stocks in rolling windows. The size of the rolling windows is chosen to be 22 which is approximately the number of trading days per month.
\end{remark}

\begin{remark}\label{ref: lag}
An important hyperparameter of the models is $\lambda$ which determines the \textit{lag} of the returns and log-realized-volatilities taken into consideration. More detailed, at every timestep $t$ we consider a rolling window of the past $\lambda$ days and collect the returns and log-realized-volatilities in this window. The data on every window will be used as explanatory variables to estimate the conditional volatility at the last day of it. 
\end{remark}

\section{Results \& Comparison}

\todo[]{Test Statistics of the Statistical Models}
\todo[]{Descriptive statistics of the returns?}
\todo[]{Specify the tunning of every model}
\todo[]{Specify the performance evaluation}

 Loss Function Comparison:
 
\begin{table}[!hbt]
\centering
\begin{tabular}{ |c|||c|c|c|c||c|c|c| } 
\hline
 & RNN&LSTM&FNN&GB&GARCH&GJR&EGARCH \\
 \hline
GSPC & 2350&2055&2132&2071&2062&\textbf{2050}&2144 \\ 
DJI & 2185&2046&2100&2051&2044&\textbf{2027}&2120\\
IXIC&2552&\textbf{2430}&2575&2467&2459&2444&2559\\
RUT&2739&\textbf{2596}&2750&2613&2614&2598&2678\\
SSMI&2029&	1975&	1993&	\textbf{1974}&	2021&	1980&	2087\\
OEX&	2155&	2091&	2142&	2092&	2086&	\textbf{2080}&	2169\\
N225&	2675&	2484&	2542&	2493&	2501&	2476&	2563\\
FTSE&	2251&	2158&	2145&	2127&	2139&	\textbf{2118}&	2194\\
\hline
\end{tabular}
\caption{Negative Log-Likelihood of the Models}
\end{table}

Root-Mean-Square-Error Comparison:

\begin{table}[!hbt]
    \centering
    \begin{tabular}{ |c|||c|c|c|c||c|c|c| } 
    \hline
     & RNN&LSTM&FNN&GB&GARCH&GJR&EGARCH \\
 \hline
GSPC&	0.639&	0.374&	0.524&	0.368&	\textbf{0.207}&	0.292&	0.393\\
DJI	&0.649	&0.452	&0.542	&0.407	&\textbf{0.22}	&0.304	&0.398\\
IXIC&	0.386&	0.459&	0.516&	0.372&	\textbf{0.196}&	0.266&	0.458\\
RUT	&0.63	&0.604	&0.691	&0.421	&\textbf{0.24}	&0.328	&0.469\\
SSMI&	0.321&	0.394&	0.355&	0.27&	\textbf{0.206}&	0.259&	0.355\\
OEX	&0.534	&0.392	&0.503	&0.34	&\textbf{0.208}	&0.291&	0.391\\
N225&	0.898&	0.436&	0.394&	\textbf{0.232}&	0.242&	0.29&	0.46\\
FTSE&	0.607&	0.496&	0.415&	0.256&	\textbf{0.178}&	0.264&	0.348\\
    \hline
    \end{tabular}
    \caption{RMSE of the Models}
\end{table}




\todo[]{comment on the results}

\chapter{Estimation of Models for Multi-Dimensional Returns}
\todo[]{Important to find a way to deal with the composite MLE or implement it by our own!!!}
\section{Introduction to Lombard Lending}
\section{Data Handling \& Model Fitting}
\section{Results \& Comparison}


\printbibliography

\end{document}
