\documentclass[a4paper, oneside]{discothesis}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{bbm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xcolor}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{mathtools} 
\usepackage{csvsimple}
\usepackage{multirow}
\usepackage{amsmath}
\setcounter{secnumdepth}{3}
\usepackage{biblatex}
\usepackage{subfig}
\usepackage{graphics}
\usepackage{diagbox}
\usepackage{lmodern}
\usepackage{mdframed}
\usepackage{multicol}





\addbibresource{references.bib}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DOCUMENT METADATA

\thesistype{Master Thesis\\ MSc Quantitative Finance UZH ETH} % Master's Thesis, Bachelor's Thesis, Semester Thesis, Group Project
\title{Comparison of Statistical and Machine Learning Methods in Modelling Time-Varying Volatility 
}

\author{Georgios Avgoustinos\\20-743-654}
\email{20-743-654}
\email{georgios.avgoustinos@uzh.ch}

\institute{Department of Banking and Finance\\
University of Zurich}

% Optionally, you can put in your own logo here
%\logo{\includegraphics[width=0.2\columnwidth]{figures/disco_logo_faded}}

\supervisors{Prof. Dr. Erich Walter Farkas\\
 Zaid Siddiqi (\textit{Zanders Treasury and Risk})}

% Optionally, keywords and categories of the work can be shown (on the Abstract page)
%\keywords{Keywords go here.}
%\categories{ACM categories go here.}

\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\frontmatter % do not remove this line
\maketitle

\cleardoublepage

\begin{acknowledgements}
	I thank Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.
\end{acknowledgements}


\begin{abstract}
    The abstract should be short, stating what you did and what the most important result is.
	Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.
\end{abstract}

\tableofcontents

\mainmatter % do not remove this line

% Start writing here
\chapter{Introduction}


\chapter{Literature Overview of Statistical Volatility Models}

\section{One Dimensional Models}\label{one_d}
Standard methods for a statistical modelling of financial returns include linear regression or Auto-regressive Moving Averages (ARMA)  models. Though, their main assumption is the existence of homoskedasticity on the residuals meaning a constant volatility throughout time, which do not seem to agree with the empirical findings. For instance, the authors in \cite{QRM} observe some stylized facts in financial returns and among others, they highlight that:
\begin{itemize}
    \item Return series are not iid although they show little serial correlation.
    \item Volatility appears to vary over time.
    \item  Extreme returns appear in clusters.
\end{itemize}

In that regard and to deal with the mentioned characteristics, in the specific section we will focus on modelling conditional volatility in one dimensional time series of returns.

We will introduce three important statistical models widely used for volatility modelling. After that, we will describe the problem formulation with the methodology used and the different models used. Finally, we will provide a detailed comparison of the studied models. The main objective is to introduce to the reader three cornerstone statistical models that will be later used as baseline models. 

To put things formally, we work in a usual probability triplet $\left( \Omega, \mathcal{F}, \mathbb{P}\right)$ equipped with the filtration $\mathbb{F} = \left(\mathcal{F}_t\right)_{t = 1, 2, \dots}$ which is generated by the available information of the market at each time step. We denote the asset's returns with the $\mathbb{F}$-adapted process  $\left(x_t\right)_{t = 1, 2, \dots}$ and our main assumption is that for $t = 1,2,\dots$ 

\begin{equation}\label{eq:1d-ass}
x_t \sim \mathcal{N}\left(\mu, \sigma_t\right)
\end{equation}

where $\sigma_t$ is a $\mathbb{F}$-predictable stochastic process denoting the conditional volatility at each time step.




  \begin{mdframed}\begin{remark}
We choose to consider the mean of the returns constant over time as we mainly focus on modelling the conditional volatility. Moreover, in the model specification and implementation later, the mean will be considered to be $0$, which is a popular choice in both single and multiple dimensions (see later) used in \cite{boulet}, \cite{fgd}, \cite{engle-multidimensions} among many others.
\end{remark}\end{mdframed}  

  \begin{mdframed}\begin{remark}
Without loss of generality, we decide to continue with the assumption that the conditional returns are normally distributed, even though Students-t extensions exist in the literature \cite{garch-t}.
\end{remark}\end{mdframed}  


\subsection{GARCH}

Generalized Autoregressive Conditional Heteroskedastic model was introduced by the author \cite{garch} and can be considered to be the starting point of volatility modelling.

\begin{definition}[GARCH $\left(p, q\right)$]\label{thm:garch_def}

$\left(x_t\right)_{t\in \mathbb{N}}$ follows a normal GARCH $\left(p, q\right)$ process with zero mean, if $\forall t \in \mathbb{N}$ holds that

\begin{gather*}\label{eq:1}
x_t = \sigma_t \epsilon_t \\
\sigma^2_t = \omega + \sum_{i=1}^p\alpha_i x_{t-i}^2 + \sum_{j=1}^q \beta_j 
\sigma_{t-j}^2  \\
\epsilon_t \sim \mathcal{N}\left(0,1\right)
\end{gather*}

\end{definition}
where $\omega > 0, \alpha_i \geq 0, i = 1,\dots,p$ and $\beta_j \geq 0, j = 1,\dots,q$

Even though, the model is simple enough, it is able to capture important features of financial returns data such as the volatility clustering. On the other hand due to the symmetry in the formula the \textit{leverage effect} cannot be captured and both upward and downward movements of the market seem to affect the volatility in the same way.

The parameters $p$ and $q$ are noting the time lag of the returns and volatility respectively.

\subsection{EGARCH}

The author \cite{egarch} introduced the exponential GARCH model as a further extension of \ref{thm:garch_def}  to deal with the leverage feature.

\begin{definition}[EGARCH $\left(p, q\right)$]\label{thm:egarch_def} $\left(x_t\right)_{t\in \mathbb{N}}$ follows a normal EGARCH $\left(p, q\right)$ process with zero mean, if $\forall t \in \mathbb{N}$ holds that

\begin{gather*}\label{eq:}
x_t = \sigma_t \epsilon_t \\
\log\sigma^2_t = \omega + \sum_{i=1}^p\alpha_i \left(|\epsilon_{t-i}|+\gamma_i\epsilon_{t-i}\right) + \sum_{j=1}^q \beta_j \log \sigma_{t-j}^2  \\
\epsilon_t \sim \mathcal{N}\left(0,1\right)
\end{gather*}

\end{definition}
where the $\gamma_i$ parameters signifies the asymmetric effects from $x_{t-i}$. 

\subsection{GJR}

In continuation with the aim of capturing better the leverage effect, the authors \cite{gjr} suggested the Glosten-Jagannathan-Runkle GARCH model. 

\begin{definition}[GJR]\label{thm:gjr} 
$\left(x_t\right)_{t\in \mathbb{N}}$ follows a normal GJR process with zero mean, if $\forall t \in \mathbb{N}$ holds that

\begin{gather*}\label{eq:}
x_t = \sigma_t \epsilon_t \\
\sigma^2_t = \omega + \left(\alpha+\gamma \mathbbm{1}_{x_{t-1}<0}\right)x_{t-1}^2+\beta \sigma_{t-1}^2 \\
\epsilon_t \sim \mathcal{N}\left(0,1\right)
\end{gather*}
\end{definition}


\subsection{Model Estimation}

For the parameter estimation of the mentioned models the method which is used is \textit{Maximum Likelihood Estimation (MLE)}.

More precisely, we denote the returns vector by $\mathbf{X}$ and without loss of generality we may consider time lag of one step. 
\begin{equation}
    f_{\bm{X}}(x_1, \dots, x_n) = f_{X_1}(x_1)\prod_{t=1}^{n-1}f_{X_{t+1}|X_t}(x_{t+1}|x_t)
\end{equation}


Denoting by $\bm{\theta}$ the model parameters, one can write the conditional variance in a general form of $\sigma_t^2 = g(\bm{\theta}, x_{t-1})$ for a measurable function $g$ which is determined from the specific model. 

Switching to the log-prices, one has
\begin{equation}
    L\left(\bm{\theta}| \bm{X}\right) = \log f_{\bm{X}}(x_1, \dots, x_n) = \log f_{X_1}(x_1) + \sum_{t=1}^{n-1} \log  f_{X_{t+1}|X_t}(x_{t+1}|x_t) 
\end{equation}

According to the normal zero-mean assumption of the returns,

\begin{equation}
\begin{split}
      L\left(\bm{\theta}| \bm{X}\right) = -\frac{1}{2}\sum_{t=1}^n \log(2\pi)+\log(\sigma_t^2)+\frac{x_t^2}{\sigma_t^2}=\\-\frac{1}{2}\left(n\log(2\pi)+\sum_{t=1}^n \log(g(\bm{\theta}, x_{t-1}))+\frac{x_t^2}{g(\bm{\theta}, x_{t-1})}\right)
\end{split}
\end{equation}

thereby, the optimal parameters can be viewed as solutions to the optimization problem:

\begin{equation}
    \bm{\theta}^* = \underset{\bm{\theta}}{\mathrm{argmin }} -L\left(\bm{\theta}| \bm{X}\right)
\end{equation}

The optimal parameters in general are a local maximum and solve the equations

\begin{equation}\label{eq:scores}
    \frac{\partial}{\partial\bm{\theta}}L\left(\bm{\theta}| \bm{X}\right) = 0
\end{equation}

where the left-hand side is known as the \textit{score vector}.  

The equation \ref{eq:scores} is solved numerically with Newton-Raphson methods. The one most typically used is the BHHH \cite{bhhh} method.

\section{Multidimensional Models}

Capturing the dependence structure of a portfolio's returns is an important manner in the fields of Asset and Risk Management. Especially, after the publication of \cite{MPT}, which signaled the start of Modern Portfolio Theory, researchers began focusing in the observation and estimation of return's covariances. 

Though, under the time varying volatility assumption the estimation of a covariance matrix in multiple dimensions and for every time step leads to huge computational complexity and inefficiency. The specific occurring problem is referred as ``curse of dimensionality'' and it stands as the main reason why the volume of studies in the multidimensional case is significantly smaller compared to the one-dimensional case.

To deal with it, the most widely used assumption from the researchers is that a part of the dependency remains the same whereas another one changes according to recursive equations similar to the one-dimensional approach.

To what follows, we will focus on the \textit{Conditional Correlation} multivariate GARCH models which stand as a simplified extension of the one-dimensional GARCH models to multiple dimensions. The specific models are also seemingly efficient during estimation especially when the number of assets is large.

In more detail, assuming that a portfolio is consisted of $d$ assets, we can again work in a probability triplet $(\Omega, \mathcal{F}, \mathbb{P})$ and denote the assets' returns as a $\mathbb{R}^d$-valued stochastic process $\mathbf{X} = (\mathbf{X}_t)_{t\in\mathbb{N}}$ and the filtration generated from them as $\mathbb{F} = (\mathcal{F})_{t\in\mathbb{N}}$. Then $\bf{X}$ is stated to follow a \textit{Conditional Correlation}-Garch process with Gaussian innovations if it is of the following form.

\begin{gather}
    \mathbf{X}_t|\mathcal{F}_{t-1}\sim\mathcal{N}(0, \Sigma_t)\label{eq: mgarch1}\\
    \Sigma_t = \Delta_t P_t \Delta_t\\
    \Delta_t = \text{diag}(\sigma_{t, 1}, \dots, \sigma_{t,p})\\
    \begin{split}\label{eq: mgarch2}
    \sigma_{t, k}^2 = \alpha_{k0}+\sum_{i=1}^{p_k}\alpha_{ki}X^2_{t-i, k}+\sum_{j=1}^{q_k}\beta_{kj}\sigma^2_{t-j, k}\text{, for }k\in\{1,\dots ,d \}\\ \text{where, } \alpha_{k0} \geq 0\text{, } \alpha_{ki}\geq0, i\in\{1, \dots, p_k\}\text{ and } \beta_{kj}\geq0, j\in\{1, \dots, q_k\}
    \end{split}\\
    P_t = f(\mathcal{F}_{t-1})
\end{gather}
for $t\in \mathbb{N}$ and a measurable function $f$ that maps to the measurable space $\left( \mathbb{R}^{d\times d}, \mathcal{B}_{\mathbb{R}^{d\times d}} \right)$.

\subsection{CCC-GARCH}
The \textit{Constant Conditional Correlation} (CCC) model was introduced by \cite{CCC} and stands as a first main attempt for modelling portfolio returns in multiple dimensions.

\begin{definition}[CCC-GARCH]
The $\mathbb{R}^d$-valued stochastic process $(\mathbf{X}_t)_{t\in\mathbb{N}}$ is a CCC-GARCH process if (i) it follows the form stated in the equations \ref{eq: mgarch1}-\ref{eq: mgarch2} and (ii) $P_t = P_c$, for $t\in \mathbb{N}$, where $P_c\in\mathbb{R}^{d\times d}$ is constant correlation matrix. 
\end{definition}

  \begin{mdframed}\begin{remark}
The model is well defined as the covariance matrix is always positive definite. Note that $\forall \mathbf{x}\neq\mathbf{0}\in\mathbb{R}^d$ one has:
\[\mathbf{x}^\intercal\Sigma_t\mathbf{x} = \left(\Delta_t\mathbf{x}\right)^\intercal P_c \left(\Delta_t\mathbf{x}\right)>0\]
since $P_c$ is a positive definite matrix and the individual volatilities $\sigma_{t, \cdot}$ that compose $\Delta_t$ are strictly positive.
\end{remark}\end{mdframed}  
In general, CCC is a useful baseline model. Though, due to its simplicity it seems to not capture well features of the market. For instance, the impact of news in the market forces a model to have a dynamic relationship for the correlation matrix which is in contrast with the CCC model.

The estimation of the model is rather simple as well and it can be performed in two steps. An useful notation for the estimation procedure is this of the \textit{devolatized} returns $\mathbf{Y}_t \coloneqq \Delta_t^{-1}\mathbf{X}_t$ as $\mathbf{Y}_t\sim \mathcal{N}(0, P_c)$.

\begin{algorithm}
\caption{Estimation of CCC model}
\begin{algorithmic}
\State \begin{enumerate}
    \item Fit $d$ individual GARCH models for each asset to estimate $\sigma_{t, k}, \forall t\in \{1, \dots, T\},\forall k\in\{1, \dots, d\}$
    \item Calculate the \textit{devolitized} returns $\mathbf{Y}_t = \Delta_t^{-1}\mathbf{X}_t$ and estimate $P_c$ as their covariance matrix, $P_c = \frac{1}{T}\sum_{t=1}^T \mathbf{Y}_t\mathbf{Y}_t^\intercal$.
\end{enumerate}
\end{algorithmic}
\end{algorithm}

\subsection{DCC-GARCH}
\textit{Dynamic Conditional Correlation} (DCC) model was introduced by \cite{DCC} and stands as a more realistic extension of CCC as the correlation matrix evolves recursively. 

For a mathematical definition of the model, it is important to define a function $\mathfrak{P}: \mathbb{R}^{d\times d}\mapsto\mathbb{R}^{d\times d}$ that maps covariance matrices to correlation matrices as follows:
\begin{equation}\label{cor_fun}
    M \longmapsto\mathfrak{P}(M) = \text{diag}\left(\frac{1}{M_{1,1}}, \dots, \frac{1}{M_{d,d}}\right)M  \text{diag}\left(\frac{1}{M_{1,1}}, \dots, \frac{1}{M_{d,d}}\right)
\end{equation}
\begin{definition}[DCC-GARCH]\label{dcc}
The $\mathbb{R}^d$-valued stochastic process $(\mathbf{X}_t)_{t\in\mathbb{N}}$ is a DCC-GARCH process if:
\begin{enumerate}
    \item  It follows the form stated in the equations \ref{eq: mgarch1}-\ref{eq: mgarch2}
    \item $P_t = \mathfrak{P}(Q_t) \text{, }\forall t\in\mathbb{N}$, with 
    \begin{gather}
        Q_t\coloneqq \left(1-\sum_{i=1}^p \alpha_i - \sum_{j=1}^q \beta_j\right)P_c+\sum_{i=1}^p\alpha_i\mathbf{Y}_{t-i}\mathbf{Y}_{t-i}^\intercal + \sum_{j=1}^q\beta_j P_{t-j}\\\mathbf{Y}_t= \Delta_t^{-1}\mathbf{X}_t\text{   (the \textit{devolatized} returns)}\\
        P_c = \frac{1}{T}\sum_{t=1}^T \mathbf{Y}_t\mathbf{Y}_t^\intercal\text{   (the constant correlation matrix)}
    \end{gather}
\end{enumerate}
\end{definition}
  \begin{mdframed}\begin{remark}\label{alpha_beta}
Note that the matrices $(Q_t)_{t\in\mathbb{N}}$ are well defined positive definite covariance matrices iff $\sum_{i=1}^p \alpha_i + \sum_{j=1}^q \beta_j<1$. In more detail,  $\forall \mathbf{x}\neq\mathbf{0}\in\mathbb{R}^d$ one has:
\scriptsize
\[\mathbf{x}^\intercal Q_t\mathbf{x} = \left(1-\sum_{i=1}^p \alpha_i - \sum_{j=1}^q \beta_j\right)\mathbf{x}^\intercal P_c\mathbf{x}+\sum_{i=1}^p\alpha_i\mathbf{x}^\intercal\mathbf{Y}_{t-i}\mathbf{Y}_{t-i}^\intercal\mathbf{x} + \sum_{j=1}^q\beta_j \mathbf{x}^\intercal P_{t-j}\mathbf{x}>0\]\[\Longleftrightarrow\]\[\sum_{i=1}^p \alpha_i + \sum_{j=1}^q \beta_j<1\]
\end{remark}\end{mdframed}  
\normalsize

Regarding the fitting of DCC model in data, maximum likelihood methods is widely used. Although, someone can directly estimate all of the parameters in one step via MLE, the most common approach is the estimation in two steps, one for the individual volatilities and one for the estimation of the conditional correlations. 

  \begin{mdframed}\begin{remark}\label{fitting CC}
For the determination of the objective function for optimization, recall that the negative log-likelihood for Gaussian residuals in a dataset $\mathbf{x} = (\mathbf{x}_t)_{t=1}^T$ is been given from:
\[l_{\mathbf{X}}(\mathbf{x}) = \frac{1}{2}\sum_{t-1}^T d\log2\pi+\log|\Sigma_t|+\mathbf{x}_t^\intercal\Sigma_t^{-1}\mathbf{x}_t\]
and according to the assumptions of the model, one has
\[l_{\mathbf{X}}(\mathbf{x}) = \frac{1}{2}\sum_{t-1}^T d\log2\pi+2\log|\Delta_t|+\log|P_t|+(\Delta_t^{-1}\mathbf{x}_t)^\intercal P_t^{-1}(\Delta_t^{-1}\mathbf{x}_t)\]
removing now the constant terms (including the individual volatilities) and switching to the \textit{devolitized} returns' notation we can define a loss function as follows:
\begin{equation}\label{eq: loss_dcc}
    L(y| \boldsymbol{\theta}) \coloneqq \sum_{t-1}^T \log|P_t(\boldsymbol{\theta})|+\mathbf{y}_t^\intercal P_t(\theta)^{-1}\mathbf{y}_t
\end{equation}
where $\boldsymbol{\theta} = (\alpha_1, \dots, \alpha_p, \beta_1, \dots, \beta_q)$.
\end{remark}\end{mdframed}  

Summarizing all of the above, the estimation of the model can be described in what follows.
\begin{algorithm}
\caption{Estimation of DCC model}
\begin{algorithmic}
\State\begin{enumerate}
    \item Fit $d$ individual GARCH models for each asset to estimate $\sigma_{t, k}, \forall t\in \{1, \dots, T\},\forall k\in\{1, \dots, d\}$.
    \item Calculate the \textit{devolitized} data $\mathbf{y}_t = \Delta_t^{-1}\mathbf{x}_t$ and estimate $P_c$ as their covariance matrix. 
    \item Solve the optimization problem using the objective function defined in \ref{eq: loss_dcc}:
    \[\theta^*\coloneqq\underset{\underset{\text{s.t. }\sum_{i=1}^p \alpha_i + \sum_{j=1}^q \beta_j<1}{\theta\in\mathbb{R}^{p+q}}}{\mathrm{argmin\text{   }     }}L(\mathbf{y}|\theta)\]
    
    \end{enumerate}
\end{algorithmic}
\end{algorithm}


\chapter{Machine Learning Models in Scope}

Machine Learning (ML) algorithms are widely used in a large variety of tasks nowadays when the computational power of modern machines has dramatically increased. 

These tasks include problems such as image classification \cite{imageNet} or sequential learning  \cite{https://doi.org/10.48550/arxiv.1706.03762} among many others.

We will introduce the reader to some of the ML algorithms that can be used for function approximation and later on provide a detailed framework for applying this basic ideas in the concept of time-varying volatility estimation.  

\section{Neural Networks}
\subsection{Feed-forward Neural Networks}\label{FNN}

Feed-forward neural networks (FNN) is a popular method used in ML and data science for both regression and classification tasks. Intuitively they can be understood as high dimensional non linear regression functions. They have their roots back in the 1940s and the main mathematical guarantee is derived from the Universal Approximation Theorem \cite{Cybenko1989ApproximationBS}. 

\theorem (Universal Approximation Theorem - G. Cybenko) Let $\sigma$ be a sigmoidal function 
\begin{equation}
\sigma(x) = \left\{ \begin{array}{@{}ll@{}}
    1, &  x\rightarrow+\infty  \\
    0, & x\rightarrow-\infty
\end{array}\right.
\end{equation}
then finite sums of the form 
\begin{equation}
g(x) = \sum_{j=1}^{N} w_j^2\sigma((w^1_j)^\intercal x+b_j)
\end{equation}
are dense in $C(I_n)$ with respect to the supremum norm. Where $x\in \mathbb{R}^n$, $w_j^1\in  \mathbb{R}^n$, $w_j^2\in \mathbb{R}$, $b_j^1\in \mathbb{R}$. $I_n$ is the $n$-dimensional unit cube $[0,1]^n$ and $C(I_n)$ is noted for the space of continuous functions on $I_n$.

In other words, given any $f\in C(I_n)$ and $\epsilon >0$, there is a sum, $g(x)$, of the above form, for which
\begin{equation}
|g(x)-f(x)|<\epsilon
\end{equation}
for all $x\in I_n$.

In what follows, we will give a definition of a FNN layer and general architecture.

\definition (Dense Layer) \label{def:nn_layer}A dense layer $\bm{z^m}$ with activation function $\phi:\mathbb{R}\mapsto\mathbb{R}$ is a function of the form
\begin{equation}
\mathbb{R}^{q_{m-1}}\ni \bm{z} \longmapsto \bm{z^m}(\bm{z}) \in \mathbb{R}^{q_{m}}
\end{equation}
Each one of the elements of the layer is called a \textit{neuron} and is of the form
\begin{equation}
\bm{z^m}(\bm{z})_j = \phi \left(w_{j,0}^m+\sum_{i=1}^{q_{m-1}}w_{j,i}^m\bm{z}_i\right)\eqqcolon\phi\left(\right(\bm{w^m}\left) ^\intercal\bm{z}\right)\text{, for } j = 1, \dots, q_m
\end{equation}
given the layer's weights $\bm{w^m}\in \mathbb{R}^{q_{m-1}+1}$. For the economy of the notation we will use $\bm{z^m}(\bm{z}) = \phi \left(\bm{W^m z} \right)$.

\definition (FNN Architecture) A Feed-forward neural network is a composition of several dense layers. In other words a function $\bm{z^{(m:1)}}$ of the form
\begin{equation}
\mathbb{R}^p\ni \bm{x} \longmapsto \bm{z^{(m:1)}}(\bm{x}) = \left(\bm{z^m}\circ\cdots\circ\bm{z^1}\right)(\bm{x})\in \mathbb{R}^q
\end{equation}
We call $p, q$ input and output dimension respectively and $m$ the depth of the network. If $m=1$ the network is called \textit{shallow} otherwise is called \textit{deep}.

\subsection{Recurrent Neural Networks}\label{RNN}
Recurrent neural networks (RNN) are models used in ML mainly for sequential learning tasks such as speech recognition \cite{speechRNN} or time series prediction \cite{Hewamalage_2021}. 

We will provide the main idea of a Simple RNN and extend it to the Long Short Term Memory Networks (LSTM) networks that follow.

We start with sequential data of the form $\bm{X} = \left(\bm{x_1}, \dots, \bm{x_T}\right)^\intercal $ where $\bm{x_t}\in\mathbb{R}^{\tau}$ - $t$ can be illustrated as a specific time step -  for $t = 1, \dots, T$. Then, a Simple RNN is consisted of a single layer of the following form similar to \ref{def:nn_layer}. 

\definition(Simple RNN Layer) A simple RNN layer is a function $\bm{z^1}$ of the following form
\begin{equation}
\begin{split}
\mathbb{R}^{\tau\times q_1} \ni (\bm{x_t}, \bm{z_{t-1}}) \longmapsto \bm{z_t} = \bm{z^1}(\bm{x_t}, \bm{z_{t-1}}) \in \mathbb{R}^{q_1} \\ \text{, for } t = 1, \dots, T
\end{split}
\end{equation}

Each node of the Simple RNN layer is of the form 
\begin{equation}
\begin{split}
z^1(\bm{x_t}, \bm{z_{t-1}})_j = z_{t,1} = \phi \left(w_{j,0}^1+\sum_{i=1}^{\tau}w_{j,i}^1x_{t,i}+\sum_{k = 1}^{q_1}u_{j,k}^1 z_{t-1, k}\right)\\\eqqcolon\phi\left(\right(\bm{w^1}\left) ^\intercal\bm{x_t}+\left(\bm{u^1}\right)^\intercal \bm{z_{t-1}}\right) \text{, for } j = 1, \dots, q_m
\end{split}
\end{equation}

Similarly, for the easing of the notation we write $\bm{z^1}(\bm{x_t}, \bm{z_{t-1}}) = \phi\left( \bm{W^1}\bm{x_t} + \bm{U^1}\bm{z_{t-1}}\right)$

\subsection{Long Short Term Memory Networks}\label{lstm}
LSTM networks are a very popular types of RNNs introduced by \cite{LSTM}. Hence, the recursive functional behaviour still appears but with a different layer mapping.

Their main advantage is that through multiple of non-linear transformations called ``gates'', the network is able to efficiently identify long and short term dependencies in the dataset.

Additionally, an important achievement of LSTM is that it is  more computationally efficient during the backpropagation stage - see \ref{ssec:num1} - as it deals with the problem of vanishing gradients. For the math behind its specific ability the reader is referred to the original paper \cite{LSTM}. 

Generally, via the use of its gates, such a model is able to summarize out the information extracted from every data point. Afterwards, based on the gate's output, the information is either remembered of forgotten in the following time steps.

A LSTM layer uses at least two different activation functions:
\begin{itemize}
    \item $\phi_\sigma = \frac{1}{1+e^{-x}} \in (0,1)$ (\textit{sigmoid function})
    \item $\phi_{\text{tanh}} = 2\phi_\sigma(2x)-1 \in (-1,1)$ (\textit{hyperbolic tangent function})
    \item $\phi: \mathbb{R}\mapsto\mathbb{R}$ a general prespecified activation function
\end{itemize}

\definition(LSTM Layer) For an eplanatory dataset of the form $\bm{X} = \left(\bm{x_1}, \dots, \bm{x_T}\right)^\intercal\in \mathbb{R}^{T\times \tau} $ the compact form of a forward pass in LSTM layer can be denoted with the following equations:

\begin{gather}
    \bf{f_t} = \phi_\sigma\left(\bf{W_f}\bf{x_t}+\bf{U_f}\bf{z_{t-1}+\bf{b_f}}\right) \text{  (forget gate)}\\
    \bf{i_t} = \phi_\sigma\left(\bf{W_i}\bf{x_t}+\bf{U_i}\bf{z_{t-1}+\bf{b_i}}\right) \text{  (input/update gate)}\\
    \bf{o_t} = \phi_\sigma\left(\bf{W_o}\bf{x_t}+\bf{U_o}\bf{z_{t-1}+\bf{b_o}}\right) \text{  (output gate)}\\
    \bf{\tilde{c}_t} = \phi_{\text{tanh}}\left(\bf{W_c}\bf{x_t}+\bf{U_c}\bf{z_{t-1}+\bf{b_c}}\right) \text{   (cell input activation)}\\
    \bf{c_t} = \bf{f_t}\bullet \bf{c_{t-1}}+\bf{i_t}\bullet\bf{\tilde{c}_t} \text{  (cell state vector)}\\
    \bf{z_t} = \bf{o_t}\bullet\phi(\bf{c_t}) \text{  (final output)}
\end{gather}

where $\bf{z_t}\in\mathbb{R}^{q_1}$ and $q_1$ is the output dimension.

\subsection{Optimization Methods for Neural Networks}\label{ssec:num1}

The methods used for estimating the layers parameters that best fit to a given data set are Gradient Descent based algorithms which are based in backpropagation \cite{Linnainmaa1976} among the layers of each neural network. For a more detailed view on backpropagation in neural networks the reader is referred to \cite{murphyMLProba}.

Though, for numerical stability and efficiency during optimization, variants of Stochastic Gradient Descent methods are widely used. More specifically, the dataset is been partitioned into disjoint subsets named \textit{batches}. Then for every \textit{batch}, the average of gradients is been calculated with respect to all of the trainable parameters and those are updated via gradient descent. The specific process takes place recursively for the batches of the dataset and once all of the available batches have been traversed one \textit{epoch} is occurred.
\section{Gradient Boosting Methods}

Gradient boosting algorithms are additive estimators consisted of multiple simple regression functions such us linear regressors or decision trees. For the scope of this master thesis we will present the algorithm the gradient boosting method with decision trees which will be later on used in the framework of conditional volatility estimation.

These method in general starts with a basic estimator for some target variable and recursively improves the estimator by performing aggregation based on the reduction of a specified loss function.

For a detailed description of regression trees the reader is referred to \cite{hastie01statisticallearning}, as the specific regression method will be only used as a part of the future models and not as an estimator by its own.

\subsection{Gradient Tree Boosting Algorithm}\label{GB}

The specific algorithm is an extension of the \textit{Functional Gradient Descent} introduced by \cite{FriedFGB}.

Suppose we have a dataset $\mathfrak{D} = \left(x_i\right)_{i=1}^N$ where $x_i\in \mathbb{R}^d$ are explanatory variables labeled with the target vector $y = \left(y_1, \dots, y_N\right)^\intercal\in \mathbb{R}^N$. We are interested in finding a parametrized function $\mathbb{R}^d \ni x \mapsto \hat{f}(x| \bf{\theta})\in \mathbb{R}$ s.t. for a specified loss function $l:\mathbb{R}^2\mapsto\mathbb{R}$, the average of losses $\frac{1}{N}\sum_{i=1}^{N}l(\hat{f}(x_i| \bf{\theta}), y_i)$ is minimized.
The proposed algorithm is the following.

\begin{algorithm}\label{gb_alg}
\caption{Gradient Tree Boosting}
\begin{algorithmic}
\State \begin{enumerate} 

\item $\text{Initialize } f_0(x) = \underset{\gamma}{\mathrm{argmin }} \sum_{i=1}^N l(y_i, \gamma)$ and learning rate $\alpha$.
\item For $m = 1, \dots, M$:

\begin{enumerate}
\item Compute for $i = 1, \dots, N$\[r_{i,m} = - \left[\frac{\partial l\left(y_i, f(x_i)\right)}{\partial f(x_i)}\right]_{f = f_{m-1}}\]

\item Fit a decision tree regressor with target variables $(r_{i,m})_{i=1}^N$ and explanatory variables $(x_i)_{i=1}^N$. Denote the tree's leaves by $R_{j, m}$, $j = 1, \dots, J_m$

\item For $j = 1,\dots, J_m$:
\[ \gamma_{j,m} = \underset{\gamma}{\mathrm{argmin }}\underset{x_i\in R_{j,m}}{\sum}l(y_i, f_{m-1}(x_i)+\gamma)\]
\item Update \[f_m(x) = f_{m-1}(x)+\alpha \sum_{j=1}^{J_m}\gamma_{j,m} \mathbbm{1}_{x\in R_{j,m}}\]

\end{enumerate}
\item Output \[\hat{f}(x) = f_M(x)\]

\end{enumerate} 
\end{algorithmic}
\end{algorithm}

\chapter{Estimation of Models for One-Dimensional Returns}

The main goal of this chapter is to replace the recursive equations for the conditional volatilities of the classical one dimensional GARCH-type models with other parametrized highly non-linear functions. Therefore, we aim to validate if the alternated models are able to capture the market structure more accurately. 

At first, we will describe the modelling assumptions. Moreover, the data handling methodology will be reviewed and after defining the models which will be used, an extended comparison between the challenger and the baseline models will follow. 

\section{Model Assumptions}

As mentioned already, we choose to follow a distributional assumption for the asset's returns. This is because, we are interested in further applications of the proposed models like in the fields of Portfolio Optimization, Market Risk Modelling and Asset Liability Management where volatility modelling plays a crucial role.

Therefore, we assume that the returns are conditionally normal distributed with time varying volatility.

For the shake of numerical stability and without loss of generality, we will work with 0-centered returns which implies that we can summarize the return's dynamics as follows.

\begin{assumption}\label{as:main1d}
We work on the probability space $\left(\Omega, \mathcal{F}, \mathbb{P}\right)$, denote the asset's returns by $\mathbf{r} = (r_1, \dots, r_T)^\intercal$ and augment the probability space with the filtration generated from the returns $\mathbb{F} = (\mathcal{F}_t)_{t=0}^T$ (where $\mathcal{F}_0$ is chosen to be the trivial $\sigma$ - algebra). We assume that for $t=1, \dots, T$, $r_{t}$ are conditionally distributed as follows:
\begin{equation}
\begin{split}
r_{t}|\mathcal{F}_{t-1}\sim \mathcal{N}(0, \sigma_{t}^2)\\
\sigma_{t} = f^{\text{model}}(\mathcal{F}_{t-1}) 
\end{split}
\end{equation}

where $f^{\text{model}}$ is a well defined function defined from the relative used model.
\end{assumption}
\section{Specification of the Loss Function}

An important step for the model estimation is the specification of the loss function for minimization.

The focus on a distributional approach for our modelling leads us to select MLE methods for the estimation of the model. According to \ref{as:main1d}, from the obtained log-likelihood of the returns, we can select the following loss function for minimization $l:\mathbb{R}^T\times\mathbb{R}^T\mapsto\mathbb{R}$.
\begin{equation}\label{eq:loss1d}
    l(\mathbf{r}, \hat{\boldsymbol{\sigma}}) = \\\frac{1}{2}\left(n\log(2\pi)+\sum_{t=1}^n \log(\hat{\sigma}_t^2)+\frac{r_t^2}{\hat{\sigma}_t^2}\right)
\end{equation}
With $\hat{\mathbf{\sigma}}\in \mathbb{R}^T$ we denote the estimated conditional volatility.

 \begin{mdframed}\begin{remark}
The specific loss function in \ref{eq:loss1d} will be used for both model fitting and performance measurement reasons throughout the chapter. 
\end{remark}\end{mdframed}       
\section{Data Handling \& Model Specification}\label{1d_meth}
\subsection{Data-set Construction}\label{dataContr}
We start with a data set containing the daily spot prices of an asset, denoted by $\mathbf{S} = (S_0, S_1, \dots, S_T)^\intercal$ and then we define the daily percentage returns as:
\begin{equation}
    \tilde{r}_{t} = 100\frac{S_{t}-S_{t-1}}{S_{t-1}} \text{,  for } t = 1, \dots, T 
\end{equation}
as mentioned we switch to the 0-centered returns:
\begin{equation}
    r_t = \tilde{r}_t - \frac{\sum_{t = 1}^{T}\tilde{r}_t}{T} \text{,  for } t = 1, \dots, T 
\end{equation}

An important definition trhoughout the section is this on of the \textit{realized volatility}.

\begin{definition}[\textit{Realized Volatility}]
For a dataset of returns $(r_t)_{t = 1}^T$ the realized volatility is given by:
\begin{equation}
    RV_t = \sqrt{\frac{1}{22}\sum_{i = t-21}^{t} \left(r_i - \frac{\sum_{j=t-21}^{t} r_j}{22}\right)^2}\text{,  for } t = 22, \dots, T 
\end{equation}
\end{definition}

To what follows we will use as additional explanatory variable for the estimation the log-realized-volatility defined as follows. 

\begin{equation}\label{eq:rv}
    \varsigma_t = \log\left(RV_t\right)\text{,  for } t = 22, \dots, T 
\end{equation}

As a consequence, we can now construct a data set of truncated size $\mathfrak{D} = \left(\mathbf{X}, \mathbf{y}\right)$, where $\mathbf{X} = (\mathbf{x_t})_{t=22+\lambda}^T$ and the target vector $\mathbf{y} = (y_t)_{t = 22+\lambda}^T$ with, 
\begin{gather}\label{data_shape}
    \mathbf{x_t} = (r_{t-\lambda}, \dots, r_{t-1}, \varsigma_{t-\lambda},\dots, \varsigma_{t-1})^\intercal\in\mathbb{R}^{2\lambda}\\
    y_t = r_t
\end{gather}
 for $ t = 23+\lambda, \dots, T$ and $\lambda>1$ (see \ref{ref: lag}).
 
 An illustration of the data handling in one rolling window can be found in Figure \ref{fig:data_handl}.
 
 \begin{figure}[H]
     \centering
     \includegraphics[width = 13cm]{figures/data_variables.png}
     \caption{Data Handling in One Rolling Window}
     \label{fig:data_handl}
 \end{figure}

To what follows, denoting with $\hat{\boldsymbol{\sigma}}(\cdot| \boldsymbol{\theta})$ the outputted volatilities from the parametrized model, we solve the following optimization problem:
\begin{equation}
    \boldsymbol{\theta}^* \coloneqq \underset{\boldsymbol{\theta}}{\mathrm{argmin}}\text{   } l(\mathbf{y}, \hat{\boldsymbol{\sigma}}(\mathbf{X}| \boldsymbol{\theta}))
\end{equation}

with $l(\cdot)$ the loss function defined in \ref{eq:loss1d}.

\begin{mdframed}\begin{remark}\label{rv_discussion}
Equation \ref{eq:rv} is derived from the realized standard deviation of the stocks in rolling windows. The size of the rolling windows is chosen to be 22 which is a popular choice (see \cite{RV_22}) as it is approximately the number of trading days per month. 
\end{remark}\end{mdframed}  

  \begin{mdframed}\begin{remark}\label{ref: lag}
An important hyperparameter of the models is $\lambda$ which determines the \textit{lag} of the returns and log-realized-volatilities taken into consideration. More detailed, at every timestep $t$ we consider a rolling window of the past $\lambda$ days and collect the returns and log-realized-volatilities in this window. The data on every window will be used as explanatory variables to estimate the conditional volatility at the following day. 
\end{remark}\end{mdframed}  


The dataset is consisted of daily returns from 8 famous stock indices covering a time period of approximately 20 years. For a detailed statistical analysis of the data, the reader is referred to \ref{ssec:1d_data} in the Appendix. After a grid search for the lag hyperparameter $\lambda$, we adjust it to 20 in order to balance the computational efficiency and the performance of every model. 

Additionally, for every index the dataset is partitioned into two subsets, namely the \textit{train set} (containing 70\% of the initial dataset) and the \textit{test set} (containing 30\% of the initial dataset). The \textit{train set} is used for parameter estimation or model fitting and the \textit{test set} for performance comparison and evaluation purposes respectively. 

\begin{mdframed}
\begin{remark}[Train-Test Split]\label{ttsplit}
Due to the inserted lags from the realized volatility (22) and the explanatory variables (20), it is worth specifying in detail how the split in a training and evaluating set takes place.

Suppose, the data of the available returns are indexed from a set of dates $I = [t_1, t_2, \dots,t_N]$. We chose to use the 70\% of the data to train our models and therefore, we can define a \textit{split date} from the index $i^*\coloneqq\lfloor{.7\times N}\rfloor$. Consequently, the \textit{train set} and \textit{test set} are indexed from the sets of dates $I_\text{train}$ and $I_\text{test}$ respectively, where:
\[I_\text{train} = [t_{43}, \dots, t_{i^*}]\]
\[I_\text{test} = [t_{i^*+43}, \dots, t_{N}]\]
\end{remark}
\end{mdframed}
\subsection{Definition of the Models}
As baseline statistical models, we select the models GARCH, EGARCH and GJR described in Section \ref{one_d}. In more detail, we estimate the model parameters of the statistical models in the \textit{train set} of every index. The detailed model estimation of these 24 models in total can be reviewed in the Figures \ref{first_8} and \ref{second_8} in the Appendix. 

Regarding the challenger models of this study we developed the models alongside with their tuning to what follows:
\begin{itemize}
    \item \textbf{RNN}:
    A recurrent neural network layer of output size 60 and \textit{tanh} activation function (as introduced in \ref{RNN}) where prior to that, data are passed through a Batch Normalization Layer  (see \ref{BNorm}). The output of the RNN layer is then passed through a dense layer of output size 1 and \textit{exponential} activation function to estimate the conditional volatility of the specific time step. Total number of trainable parameters for the data input described in \ref{data_shape} is 3,849.
    
    \item \textbf{LSTM}:
    A long short term memory layer of output size 60 and \textit{tanh} activation function at the final output (as introduced in \ref{lstm}). Similarly, prior to that data are passed through a Batch Normalization Layer and the output of the LSTM is passed through a Dense Layer of \textit{exponential} activation function. The total number of trainable parameters id 15,189.
    
    \item \textbf{FNN}: 
    A Feed-Forward neural network (as described in \ref{FNN}) of hidden size 300 and output size 1 with activation functions \textit{tanh} and \textit{exponential} respectively. For numerical efficiency, the input of the model was again passed through a Batch Normalization Layer as well.
    
    \item \textbf{GB}:
    A Gradient Tree Boosting estimator as described in \ref{GB}. The number of leaves for the individual small regression trees is fixed to 2, the learning rate $\alpha = .2$ and the number of individual trees $M = 200$. 
    
\end{itemize}


For the fit of the specified models to the data the loss function defined in \ref{eq:loss1d} is used as an objective function for minimization.

The models RNN and LSTM are trained for 10 epochs with a batch size of 2048. For the optimization problem, a Stochastic Gradient Descent variant method was used namely the ADAM algorithm, first introduced by \cite{Adam}. The hyperparameter of the learning rate for the optimizer was adjusted to .008.

FNN model is trained for 20 epochs with a bach size of 1024. Similarly with before, the optimizer selected is ADAM with a fixed learning rate of .001.

Last but not least, for the GB model the Algorithm \ref{gb_alg} is been applied with 2 leaves for every small regression tree, a learning rate $\alpha = .2$ and aggregated over $M=200$ regression trees.

\begin{mdframed}\begin{remark}[Batch Normalization Layer]\label{BNorm}
The specific layer, applies an affine transformation to a given batch of data. In more detail, for a batch of data $\mathfrak{B}$ with sample mean $\mu_{\mathfrak{B}}$ and sample standard deviation $\sigma_{\mathfrak{B}}$, the layer can be seen as function $BN$ where:
\[
\mathfrak{B}\longmapsto BN(\mathfrak{B}| \gamma, \beta) = \gamma\frac{\mathfrak{B}-\mu_\mathfrak{B}}{\sigma_{\mathfrak{B}}}+\beta
\]
where, $\gamma$ and $\beta$ are trainable parameters during the optimization.

The Batch Normalization Layer is mainly used for scaling batches of data close to 0 mean and 1 standard deviation for numerical stability during the backprobagation stage of the training of neural networks and it was first introduced by \cite{BNorm}.
\end{remark}\end{mdframed}

\section{Results \& Comparison}

\subsection{Performance Evaluation}
The performance evaluation of the models is been held with two different approaches.

Firstly, we compare the goodness of fit by following all of the modelling assumptions. In that regard, after optimizing the parameters of the models in the training set, we estimate the conditional volatility for every time step of the test set. Consequently, with the the label vector on the test set $\mathbf{y}_\text{test}$ (containing the realized returns) and the estimated conditional volatility vector $\boldsymbol{\hat{\sigma}}_\text{test}$ (output from the model) we calculate $l(\mathbf{y}_\text{test}, \boldsymbol{\hat{\sigma}}_\text{test})$ where $l$ is defined in \ref{eq:loss1d}. The results from this comparison can be illustrated in Table \ref{nll_results_1d}. 

\begin{mdframed}
\begin{remark}
Note that the loss function described in \ref{eq:loss1d} is not the actual negative log-likelihood of the data. This can be validated if one compares the values of the statistical models in the table \ref{nll_results_1d} and in the Figures \ref{first_8} and \ref{second_8} in the Appendix. This small miss-match occurs from the fact that even that the returns are 0-centered, the MLE estimate for the constant mean $\mu$ is not necessarily 0.

For instance, for the negative log-likelihood of a constant mean - conditionally heteroscedastic model, regarding the score function of $\mu$ one has:
\[\text{nll}(\mathbf{r}| \mu, \boldsymbol{\sigma}) = \frac{1}{2}\left(T\log(2\pi)+\sum_{t} \log(\sigma_t^2) + \left(\frac{r_t-\mu}{\sigma_t}\right)^2\right) \]
\[
\frac{\partial \text{nll}}{\partial \mu} = 0 \Leftrightarrow \mu = \frac{\sum_t r_t/\sigma_t^2}{\sum_t 1/\sigma_t^2}
\]
Although, as already discussed, the selected loss function in \ref{eq:loss1d} is a widely used choice among researchers, in the setup of modelling time varying volatility in financial returns. The mathematical motivation behind it is the conditional martingale assumption of the returns that leads for a 0 constant mean $\mu$. Moreover, due to the small deviation of the daily percentage returns around zero, the modelling dwells on capturing the volatility of the market instead.
\end{remark}
\end{mdframed}

On the contrary to the above loss comparison, we also select another performance evaluation independent with our modelling assumptions. In more detail, we define the \textit{realized volatility} of the data in what follows.



Consequently, we decide to compare the estimated conditional volatility from the models to the realized volatility with the Root Mean Square Error (RMSE) loss function. The results then are evaluated on the \textit{test set} via 

\begin{equation}
    RMSE(\hat{\boldsymbol{\sigma}}_{test}, \mathbf{RV}_{test}) = \left\|\frac{\hat{\boldsymbol{\sigma}}_{test}-\mathbf{RV}_{test}}{\sqrt{N_{test}-22}}\right\|_2
\end{equation}
where, $N_{test}$ denotes the cardinality of the \textit{test set} and $\|\cdot\|$ the Euclidean Norm on $\mathbb{R}^{N_{test}}$.

The results of the second comparison can be visualized in Table \ref{RMSE_1d}.
\begin{table}[!ht]
    \centering
    \scriptsize
    \begin{tabular}{|p{2.7cm}||p{.7cm}|p{.7cm}|p{.7cm}|p{.7cm}|p{.7cm}|p{.7cm}|p{.7cm}|p{.7cm}|}
    \hline
         \backslashbox{MODEL}{INDEX}& GSPC & DJI & IXIC & RUT & SSMI & OEX & N225 & FTSE \\ \hline\hline
        RNN & 2125 & 2089 & 2532 & 2701 & 2035 & 2166 & 2525 & 2164 \\ \hline
        LSTM & 2082 & 2062 & \textbf{2458} & \textbf{2607} & \textbf{1981} & 2114 & \textbf{2484} & \textbf{2130} \\ \hline
        FNN & 2146 & 2129 & 2865 & 2706 & 2011 & 2156 & 2533 & 2157 \\ \hline
        GB & 2090 & 2069 & 2494 & 2643 & 1999 & 2113 & 2499 & 2163 \\ \hline
        GARCH & 2086 & 2065 & 2487 & 2637 & 2037 & 2110 & 2513 & 2158 \\ \hline
        GJR & \textbf{2075} & \textbf{2049} & 2472 & 2622 & 1998 & \textbf{2106} & 2489 & 2137 \\ \hline
        EGARCH & 2168 & 2141 & 2584 & 2700 & 2111 & 2194 & 2580 & 2217 \\ \hline
    \end{tabular}
    \normalsize
    \caption{Loss Function of the Models in the Test Set}
    \label{nll_results_1d}
\end{table}

\begin{table}[!ht]
    \centering
    \scriptsize
    \begin{tabular}{|p{2.7cm}||p{.7cm}|p{.7cm}|p{.7cm}|p{.7cm}|p{.7cm}|p{.7cm}|p{.7cm}|p{.7cm}|}
    \hline
         \backslashbox{MODEL}{INDEX}& GSPC & DJI & IXIC & RUT & SSMI & OEX & N225 & FTSE \\ \hline\hline
        RNN & 0.471 & 0.422 & 0.721 & 0.579 & 0.356 & 0.425 & 0.390 & 0.364 \\ \hline
        LSTM & 0.312 & 0.311 & 0.304 & 0.442 & 0.337 & 0.293 & 0.364 & 0.354 \\ \hline
        FNN & 0.500 & 0.570 & 0.867 & 0.563 & 0.362 & 0.495 & 0.398 & 0.404 \\ \hline
        GB & 0.347 & 0.386 & 0.378 & 0.424 & 0.263 & 0.356 & \textbf{0.235} & 0.241 \\ \hline
        GARCH & \textbf{0.219} & \textbf{0.230} & \textbf{0.207} & \textbf{0.245} & \textbf{0.207} & \textbf{0.220} & 0.241 & \textbf{0.195} \\ \hline
        GJR & 0.301 & 0.310 & 0.275 & 0.334 & 0.263 & 0.300 & 0.294 & 0.281 \\ \hline
        EGARCH & 0.403 & 0.406 & 0.466 & 0.474 & 0.369 & 0.401 & 0.474 & 0.367 \\ \hline
    \end{tabular}
    \normalsize
        \caption{$RMSE$ of the Models}
        \label{RMSE_1d}
\end{table}

Admittedly, the running times for the training of the models can be found in Table \ref{running_times}. Note that the models where trained in a laptop equipped with  an Intel Core i7-1165G7 (12MB Cache, up to 4.7GHz) processor.

\begin{table}[H]
    \scriptsize
    \centering
    \begin{tabular}{|p{2.7cm}||p{.7cm}|p{.7cm}|p{.7cm}|p{.7cm}|p{.7cm}|p{.7cm}|p{.7cm}|p{.7cm}|}
    \hline
        \backslashbox{MODEL}{INDEX} & GSPC & DJI & IXIC & RUT & SSMI & OEX & N225 & FTSE \\ \hline\hline
        RNN & 1.974 & 1.730 & 1.904 & 1.749 & 1.880 & 1.895 & 1.900 & 2.269 \\ \hline
        LSTM & 5.685 & 5.750 & 5.850 & 6.170 & 6.159 & 6.379 & 6.005 & 6.470 \\ \hline
        FNN & 0.840 & 0.852 & 0.938 & 0.906 & 0.925 & 1.020 & 0.850 & 0.870 \\ \hline
        GB & 0.060 & 0.047 & 0.070 & 0.056 & 0.075 & 0.060 & 0.097 & 0.036 \\ \hline
        GARCH & 0.040 & 0.018 & 0.029 & 0.020 & 0.025 & 0.025 & 0.032 & 0.020 \\ \hline
        GJR & 0.034 & 0.034 & 0.035 & 0.030 & 0.032 & 0.045 & 0.032 & 0.032 \\ \hline
        EGARCH & 0.040 & 0.035 & 0.049 & 0.048 & 0.035 & 0.050 & 0.035 & 0.040 \\ \hline
    \end{tabular}
    \normalsize
    \caption{Running Times}
    \label{running_times}
\end{table}

\subsection{Comments on the Results}

As can be concluded, regarding the loss comparison the ML models stand out with a marginal difference for most of the indices. From the other hand, when we compare the output with the realized volatility, the statistical GARCH and GJR outperform the ML models.

The main reasoning behind it, is that due to the significantly increased number of the ML models' parameters, the variance of the estimators increases. 

In more detail, even though the $RV$ as defined is a quite smooth function (especially for a 22 days rolling window), the output of the ML models is not. The loss function $l$ defined in \ref{eq:loss1d} inherits all of the noise of the data-set and that leads to estimators for the trainable parameters which may minimize better the objective function, but produce additional noise in the estimator. 

Regarding an inside comparison of the ML models, as expected from previous studies, the ability of the LSTM layers to differentiate which past information is more beneficial to keep, renders the LSTM model the best ML model.

Last but not least, it is worth mentioning that specific evaluation selection based on the $RMSE$ (given the validity of the \textit{realized volatility} as an index that captures the market's characteristics correctly) captures the model risk of our methodology. Meaning that there is irreducible error due to the Gaussian assumption of the returns and the general modelling approach.





\chapter{Estimation of Models for Multi-Dimensional Returns}

At the specific chapter we try to expand the ideas on modelling time varying volatility, in a multidimensional environment. By combining Conditional Correlation models with ML and statistical models for one dimensional time series, we perform similar work with Chapter 4, with the difference that the conditional volatility is replaced by conditional covariance matrix.

To what follows, we will introduce the reader with the main modeling approach and assumptions, discuss about the specification of a suitable loss function and derive the results for comparison.


\section{Model Assumptions}

Similarly with the one dimensional case, the distributional approach remains. The main goal is to model a portfolio of assets, by assuming that its returns are conditionally Normal distributed around zero.

\begin{assumption}
Working in a probability space $(\Omega, \mathcal{F}, \mathbb{P})$, we denote a portfolio's returns consisted of $d$ assets by $R = (\mathbf{R_1}, \dots, \mathbf{R_T})$ and the filtration generated from them by $\mathbb{F} = (\mathcal{F}_t)_{t=0}^T$ (where $\mathcal{F}_0$ is chosen to be the trivial $\sigma$-algebra). Therefore, we assume that the portfolio returns obey the follow distributional dynamics:

\begin{equation}\label{dynamics_md}
\left.
    \begin{matrix}
        \mathbf{R}_t|\mathcal{F}_{t-1} \sim \mathcal{N}\left(\mathbf{0}, \Sigma_t\right)\\ 
        \Sigma_t = \Delta_t P_t \Delta_t \\
        P_t = \mathfrak{P}\left( \left(1-\alpha -  \beta \right)P_c+\alpha\mathbf{Y}_{t-1}\mathbf{Y}_{t-1}^\intercal + \beta P_{t-1}\right) \\ 
        \mathbf{Y}_t = \Delta_t^{-1}\mathbf{R}_t\\
        P_c = \frac{1}{T}\sum_{t=1}^T \mathbf{Y}_t\mathbf{Y}_t^\intercal\\
        \Delta_t = \text{diag}(\sigma_{t,1}, \dots, \sigma_{t,d}) \\ 
        \sigma_{t,j} = f_j(\mathcal{F}_{t-1})\text{     , for $j = 1, \dots, d$}
    \end{matrix}       \right\} \text{     , for $t=1, \dots, T$ }
\end{equation}
\end{assumption}

The model definition is inspired from the definition of the DCC models in Definition \ref{dcc} the only difference is that now the functions $f_j$ are generic measurable functions and not necessarily the GARCH conditional volatility functions. For recapitulation reasons, $\mathfrak{P}$ is the correlation function that maps covariance matrices to correlation matrices defined in \ref{cor_fun}, $\mathbf{Y}_t$ and $P_c$ denote the devolatized returns and the constant correlation matrix respectively. Last but not least, for the additional trainable parameters we have \[(\alpha, \beta)\in (0,1)\times (0,1)\text{     s.t. } \alpha+\beta<1.\]

Similarly with the one dimensional case, $\mathbf{0}$-centered returns are assumed. The reasoning behind is that, it is again a very popular assumption when modelling daily portfolio returns and the research focuses mainly in the volatility estimation rather than the mean of the portfolio.

The main idea behind the modelling approach is to work with hybrid models for the portfolios. In more detail, we estimate the individual conditional volatilities for each asset in the portfolio via a model described in Chapter 4 and then we further assume the Conditional Correlation dynamics for the dependence structure of the portfolio to find the parameters $(\alpha, \beta)$ that further minimize our loss function.

\section{Specification of the Loss Function}
Similarly with the previous chapter, the distributional approach motivates us to use MLE methods for the estimation of the models. Therefore, we can switch and minimize the negative log likelihood instead. Inspired by the way the statistical conditional correlation models are estimated (described in Remark \ref{fitting CC}) we proceed as follows. We denote the estimated correlation matrices by $\mathbf{\widehat{P}} = \left(\widehat{P}_t\right)_{t=1}^T$ and the targeted devolatized returns by $\mathbf{Y} = \left(\mathbf{Y}_t\right)_{t=1}^T$ for $t = 1, \dots, T$ we define the loss function $L:\mathbb{R}^{T\times d\times d}\times \mathbb{R}^{T\times d}\mapsto \mathbb{R}$ as:
\begin{equation}\label{md_loss}
    \left(\mathbf{\widehat{P}}, \mathbf{Y}\right)\longmapsto L\left(\mathbf{\widehat{P}}, \mathbf{Y}\right) \coloneqq \frac{1}{2}\sum_{t=1}^T \log \left|\widehat{P}_t\right| + \mathbf{Y}_t^\intercal \widehat{P}_t^{-1} \mathbf{Y}_t
\end{equation}

\section{Data Handling \& Model Fitting}

For the comparison purposes we chose to construct 3 portfolios of 19 stocks, each one equally weighted. In more detail, we constructed the portfolios with stocks listed on the famous Berkshire Hathaway portfolio (BH), Financial Times Stock Exchange 100 (FTSE) and Swiss Market Index (SMI). Figures for describing the consistency of these portfolios and the time horizon can be found in Subsection \ref{Multi_data} of the Appendix.  

The estimated models are characterized by two components. Firstly, it is the individual volatility structure that is assumed to rule every different asset listed in the portfolio. Secondly, the correlation structure of the portfolios throughout time which is uniquely determined from the parameters $(\alpha, \beta)$ of the Conditional Correlation model.

Therefore, the estimation of this models takes part in two stages. First, for every individual asset we fit the one-dimensional volatility models (3 Statistical and 3 ML) according to the methodology rigorously described in Section \ref{1d_meth}. Afterwards, after obtaining the individual estimated conditional volatilities, we are further optimizing the DCC parameters in order to minimize the negative log likelihood of the data. In more detail, we proceed with the following optimization problem
\begin{equation}
    (\alpha^*, \beta^*)\coloneqq \underset{\underset{\text{s.t. } \alpha+\beta<1}{(\alpha, \beta)\in (0,1)^2}}{\text{argmin   }} L\left(\mathbf{\hat{P}}(\alpha, \beta), \mathbf{Y}\right)
\end{equation}
 where the loss function $L\left(\cdot, \cdot\right)$ is given in \ref{md_loss} and the structure of the 3 dimensional $\mathbf{\hat{P}}(\alpha, \beta)$ for the conditional correlation of every step, is described in the dynamics of \ref{dynamics_md}. The optimization is been with the Sequential Least Squares Programming (SLSQP) algorithm.
 
 For illustrative purposes, the reader may refer to the Additional Figures section of the Appendix for an illustration of such a loss surface in Figure \ref{dcc_surf}. Recall that the linear constraint for $\alpha$ and $\beta$ is there to assure the well-posedness of the covariance matrices (see Remark \ref{alpha_beta}).

\section{Results \& Comparison}
\subsection{Performance Evaluation}
For the evaluation stage, we choose to split the available datasets to train and test according to the methodology described in Remark \ref{ttsplit}. Therefore, to what follows we are estimating the model based on the available data of the train set and evaluate its performance on the data of the test set.

In parallel to the one-dimensional comparison, we would like to additionally evaluate the performance of the models in a model-free setup except from the Negative-Log-Likelihood comparison. In that regard, we will compare the RMSE of the estimated conditional volatility from the models in scope with what we define as \textit{Realized Portfolio Volatility}. 

\begin{definition}(\textit{Realized Portfolio Volatility} (RPV))
Assume an equally weighted  portfolio consisted of $d$ different assets . We denote with $r^j_t$ for $t = 1, \dots, T$ and $j = 1,\dots, d$ the percentage return of asset $j$ at time $t$. Then we define the RPV as follows:
\begin{equation}
    RPV_t = \frac{1}{d}\sqrt{\mathbf{e_d}^\intercal \left(\frac{\sum_{i=t-21}^t \mathbf{r}_{i}\mathbf{r}_{i}^\intercal}{22}\right)\mathbf{e_d}} \text{, for } t = 22,\dots, T
\end{equation}
where we denote with $\mathbf{r_i}\coloneqq(r_i^1, \dots, r_i^d)^\intercal$ and $\mathbf{e_d} \coloneqq (1, \dots, 1)^\intercal \in \mathbb{R}^d$.
\end{definition}

Recall that the models now output an estimation of the conditional covariance matrix for each time step $\widehat{\Sigma}_t$. Therefore we can - under the 1 share position for every asset assumption - denote the \textit{Model Portfolio Volatility} (MPV) as: 
\begin{equation}
    \mathbf{MPV} = \frac{1}{d} \left(\sqrt{\mathbf{e_d}^\intercal \widehat{\Sigma}_{22} \mathbf{e_d}}, \dots, \sqrt{\mathbf{e_d}^\intercal \widehat{\Sigma}_T \mathbf{e_d}}\right)^\intercal
\end{equation}  
Finally, we can now have an additional performance measurement in set - as an extension of the Realized Volatility handling of the One-Dimensional case - calculated from the the Root-Mean-Square-Error
\begin{equation}
    RMSE\left(\mathbf{MPV}_{\text{set}}, \mathbf{RPV}_\text{set}\right) =  \left\|\frac{\mathbf{MPV}_{\text{set}}-\mathbf{RPV}_\text{set}}{\sqrt{N_{set}-22}}\right\|_2 
\end{equation}.

To what follows, the comparison results for both approaches are illustrated in the following Tables \ref{tbl_md_1}, \ref{tbl_md_2}, \ref{tbl_md_3}.

\subsection{Comments on the Results}
\begin{table}[!ht]\label{tbl_md_1}
    \scriptsize
    \centering
    \begin{tabular}{|p{3.5cm}||p{1.1cm}|p{1.1cm}|p{1.1cm}|p{1.1cm}||p{1.1cm}|p{1.1cm}|}
    \hline
     & RMSE TRAIN SET & NLL TRAIN SET & RMSE TEST SET & NLL TEST SET & alpha & beta \\ \hline\hline
        Dynamic CC - GARCH & 0.152 & 8983 & 0.283 & -426 & 0.108 & 0.883 \\ \hline
        Constant CC - GARCH & 0.260 & 22860 & 0.451 & 3651 & 0.000 & 0.000 \\ \hline
        Dynamic CC - EGARCH & 0.189 & 9666 & 0.375 & 4025 & 0.146 & 0.786 \\ \hline
        Constant CC - EGARCH & 0.257 & 22647 & 0.481 & 6569 & 0.000 & 0.000 \\ \hline
        Dynamic CC - GJR & 0.153 & 8832 & 0.292 & -663 & 0.106 & 0.886 \\ \hline
        Constant CC - GJR & 0.255 & 22783 & 0.453 & 3462 & 0.000 & 0.000 \\ \hline
        Dynamic CC - GB & 0.235 & 8096 & 0.609 & 991 & 0.185 & 0.498 \\ \hline
        Constant CC - GB & 0.269 & 21332 & 0.654 & 2133 & 0.000 & 0.000 \\ \hline
        Dynamic CC - FNN & 0.242 & 9335 & 0.687 & -546 & 0.122 & 0.861 \\ \hline
        Constant CC - FNN & 0.316 & 23353 & 0.741 & 239 & 0.000 & 0.000 \\ \hline
        Dynamic CC - LSTM & 0.227 & 9276 & 0.440 & 1070 & 0.173 & 0.692 \\ \hline
        Constant CC - LSTM & 0.271 & 22268 & 0.484 & 3265 & 0.000 & 0.000 \\ \hline
    \end{tabular}
            \normalsize

\end{table}

\begin{table}[!ht]\label{tbl_md_2}
    \centering
    \scriptsize
    \begin{tabular}{|p{3.5cm}||p{1.1cm}|p{1.1cm}|p{1.1cm}|p{1.1cm}||p{1.1cm}|p{1.1cm}|}
    \hline
      & RMSE TRAIN SET & NLL TRAIN SET & RMSE TEST SET & NLL TEST SET & alpha & beta \\ \hline\hline
        Dynamic CC - GARCH & 0.211 & 11694 & 0.578 & 1867 & 0.114 & 0.862 \\ \hline
        Constant CC - GARCH & 0.278 & 20316 & 0.671 & 3903 & 0.000 & 0.000 \\ \hline
        Dynamic CC - EGARCH & 0.212 & 11697 & 0.623 & 2753 & 0.104 & 0.884 \\ \hline
        Constant CC - EGARCH & 0.284 & 20235 & 0.732 & 4984 & 0.000 & 0.000 \\ \hline
        Dynamic CC - GJR & 0.209 & 11352 & 0.620 & 1443 & 0.106 & 0.883 \\ \hline
        Constant CC - GJR & 0.279 & 20237 & 0.708 & 3883 & 0.000 & 0.000 \\ \hline
        Dynamic CC - GB & 0.231 & 7565 & 0.927 & -455 & 0.147 & 0.837 \\ \hline
        Constant CC - GB & 0.303 & 18829 & 0.978 & 2257 & 0.000 & 0.000 \\ \hline
        Dynamic CC - FNN & 0.301 & 13854 & 0.886 & 3067 & 0.132 & 0.832 \\ \hline
        Constant CC - FNN & 0.306 & 20845 & 0.975 & 3146 & 0.000 & 0.000 \\ \hline
        Dynamic CC - LSTM & 0.272 & 10777 & 0.746 & 2059 & 0.180 & 0.696 \\ \hline
        Constant CC - LSTM & 0.295 & 19848 & 0.787 & 3527 & 0.000 & 0.000 \\ \hline
    \end{tabular}
\end{table}

\begin{table}[!ht]\label{tbl_md_3}
    \centering
    \scriptsize
    \begin{tabular}{|p{3.5cm}||p{1.1cm}|p{1.1cm}|p{1.1cm}|p{1.1cm}||p{1.1cm}|p{1.1cm}|}
    \hline
         & RMSE TRAIN SET & NLL TRAIN SET & RMSE TEST SET & NLL TEST SET & alpha & beta \\ \hline\hline
        Dynamic CC - GARCH & 0.204 & 4375 & 0.266 & -4603 & 0.117 & 0.875 \\ \hline
        Constant CC - GARCH & 0.301 & 19927 & 0.359 & 11 & 0.000 & 0.000 \\ \hline
        Dynamic CC - EGARCH & 0.225 & 4720 & 0.352 & -1804 & 0.115 & 0.876 \\ \hline
        Constant CC - EGARCH & 0.311 & 19947 & 0.430 & 2630 & 0.000 & 0.000 \\ \hline
        Dynamic CC - GJR & 0.247 & 6015 & 0.349 & -2696 & 0.143 & 0.787 \\ \hline
        Constant CC - GJR & 0.299 & 20040 & 0.385 & -4 & 0.000 & 0.000 \\ \hline
        Dynamic CC - GB & 0.283 & 3501 & 0.452 & -3518 & 0.215 & 0.657 \\ \hline
        Constant CC - GB & 0.330 & 18624 & 0.492 & -780 & 0.000 & 0.000 \\ \hline
        Dynamic CC - FNN & 0.280 & 5288 & 0.491 & -4006 & 0.133 & 0.850 \\ \hline
        Constant CC - FNN & 0.361 & 20064 & 0.561 & -460 & 0.000 & 0.000 \\ \hline
        Dynamic CC - LSTM & 0.302 & 6256 & 0.423 & -2207 & 0.188 & 0.639 \\ \hline
        Constant CC - LSTM & 0.327 & 19727 & 0.434 & -116 & 0.000 & 0.000 \\ \hline
    \end{tabular}
\end{table}

\chapter{Lombard Lending Applications}\label{Lombard}

To designate the importance of an accurate market volatility estimation, we will provide some applications of the already covered material in the framework of Lombard Lending.

Therefore, in the last chapter of this dissertation, we will introduce the reader to the general concept of a Lombard loan and with the toolkit already developed in the previous chapters, we will develop two modelling approaches for real world applications in the field.

\section{Definition of Lombard Loans}

Lombard Lending specifically in Switzerland stands as a typical definition for security-backed lending. In more detail, the obligor gets access to funding, by providing a collateral which has a dynamic market value and can be traded in a liquid environment. Typical examples of such collateral may include portfolios of stocks, bonds, funds, structured products, etc. As an aftermath the lender gets exposure to market risk from the fluctuations of the collateral backing the exposure. 

The credit limit up to which the borrower can withdraw capital is variable and characterized as a percentage of the running collateral's value named \textit{lending value}. In the case of a default from the counterparty, the institution is agreed to have the option to liquidate the assets in order to recover the losses.

The event when the running collateral's value drops below the running liabilities amount is called \textit{breach}. Even though the lender can liquidate the securities after a \textit{breach} occurs, in practice she does not. Instead, she follows a control policy named \textit{margin call} during which she asks the borrower to either (i) reduce her exposure or to (ii) provide additional collateral during a period of specified number of days called \textit{closeout period}.

As an aftermath, the emerging market risk from the collateral's fluctuations obliges the institution to have the necessary quantitative tools not only to price such a loan, but also to monitor it during its lifespan and have early warning estimates for potential margin calls. During the following subsections \ref{signal} and \ref{lendingValue} we will provide simplified application regarding both of these operations.


\section{Risk Signal Application}\label{signal}

For the scope of this application we would like to construct a monitoring tool during the life of the loan. In more detail, at every timestep given the available market history we calculate the conditional probability of having a \textit{breach}, named \textit{breach probability}. In what follows we will use the the methodology described in Chapter 5 for the dynamics of the portfolio returns and more specifically focus on the best statistical and ML models, the DCC-GARCH and DCC-LSTM respectively.

 Additionally, given the estimated model at every time we will provide a method for forecasting the covariance matrix for a short future time horizon in order to estimate the forecasted \textit{breach probabilities}.

To start with a mathematical modelling of the problem we denote with $T$ the maturity of the loan and we work on a discretized time grid (of days) $t = 0, \dots, T$. We also work on the probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and assume that the collateral portfolio is consisted of $d$ assets of which the percentage returns are denoted by the $\mathbb{R}^d$-valued stochastic process $\mathbf{R} = (\mathbf{R}_t)_{t = 0, \dots, T}$ which is assumed to obey the distributional dynamics stated in \ref{dynamics_md}. Moreover, we augment $\mathcal{F}$ with the filtration $\mathbb{F}=(\mathcal{F}_t)_{t = 0, \dots, T}$ generated from $\mathbf{R}$. The value of the portfolio is denoted by the $\mathbb{R}_+$-valued $\mathbb{F}$-adapted stochastic process $V = (V_t)_{t=0, \dots, T}$ which is dependent on the portfolios weight $\mathbf{w}\in \mathbb{R}^d$ and the $\mathbb{R}^d$-valued $\mathbb{F}$-adapted stochastic process $\mathbf{S} = (\mathbf{S}_t)_{t=0, \dots, T}$ of the assets' prices. Last but not least, we notate the liabilities amount of the obligor by the $\mathbb{R}_+$-valued $\mathbb{F}$-predictable stochastic process $L = (L_t)_{t=0, \dots, T}$.

\begin{mdframed}
\begin{remark}
Note here that $\mathcal{F}_0$ is not necessarily the trivial sigma algebra as the assets of the portfolio may have existed before the inception of the loan ($t=0$), therefore $\mathcal{F}_0$ inherits all the realized information from the past. In order not to use negative indexing for the time, we stick with this notation. 
\end{remark}
\end{mdframed}

\begin{mdframed}
\begin{remark}\label{weights}
We chose to assume the portfolio weights $\mathbf{w}$ to be constant, but without any change in the rest of our methodology, someone could model the portfolio weights with a $\mathbb{F}$-predictable $\mathbb{R}^d$-valued stochastic process $(\mathbf{w}_t)_{t = 0, \dots, T}$. That can be especially useful for portfolio optimization or hedging applications.
\end{remark}
\end{mdframed}

\subsection{Breach Probability Calculation}
Collecting all of the above definitions and assumptions, we can denote the \textit{breach probability} at time $t$ by ${BP}_t$ for $t = 0, \dots, T-1$ and to what follows one has.
\begin{equation}\label{BP}
    \begin{split}
{BP}_{t} = \mathbb{P}\left(V_{t+1}<L_{t+1}|\mathcal{F}_t\right) = \mathbb{P}\left(\mathbf{w}^\intercal \mathbf{S}_{t+1}<L_{t+1}|\mathcal{F}_t \right)\\
=\mathbb{P}\left(\mathbf{w}^\intercal\left[\mathbf{S}_t \odot\left(\mathbf{e_d}+\mathbf{R}_{t+1}\right)\right]<L_{t+1}|\mathcal{F}_t\right)  \\
=\mathbb{P}\left(\mathbf{w}^\intercal D^\mathbf{S}_t\left(\mathbf{e_d}+\mathbf{R}_{t+1}\right)<L_{t+1}| \mathcal{F}_t\right)\\
=\mathbb{P}\left(\mathbf{w}^\intercal\left( D^\mathbf{S}_t\mathbf{e_d}\right)+\mathbf{w}^\intercal D^\mathbf{S}_t\Sigma_{t+1} (D^\mathbf{S}_t)^\intercal \mathbf{w} Z<L_{t+1}| \mathcal{F}_t\right)\\
=\Phi\left(\frac{L_{t+1}-\mathbf{w}^\intercal D^\mathbf{S}_t\mathbf{e_d}}{\mathbf{w}^\intercal D^\mathbf{S}_t\Sigma_{t+1} (D^\mathbf{S}_t)^\intercal \mathbf{w}}\right)
    \end{split}
\end{equation}

Where, 
\begin{itemize}
    \item $\odot$ denotes the elementwise multiplication.
    \item $\mathbf{e_d} = (1, \dots, 1)^\intercal\in\mathbb{R}^d$.
    \item $D^\mathbf{S}_t = \text{diag}\left(\mathbf{S}_t^1, \dots, \mathbf{S}_t^d\right)$ for $t = 0, \dots, T-1$
    \item $Z\sim\mathcal{N}(0,1)$.
    \item $\Phi(\cdot)$ the c.d.f. of the standard normal distribution.
\end{itemize}

We implemented the proposed methodology for a collateral portfolio of a private bank, which can be briefly illustrated in Figure \ref{ll_portf}. Because we would like to demonstrate the outputed \textit{breach probabilities} we specifically chose to fit the DCC models up to a stressed day for the market and the loan 11/02/20. Moreover, we assumed that the running exposure is 90\% of the previous day collateral's value. The time series of the collateral's value together with the exposure and the outputted in sample \textit{breach probability} can be found in Figure  \ref{fig:bp}.


\begin{figure}[h]
    \centering
    \subfloat[\centering GARCH]{{\includegraphics[width=6cm]{figures/garch_bp_insample.png} }}
    \qquad
    \subfloat[\centering LSTM]{{\includegraphics[width=6cm]{figures/lstm_bp_insample.png} }}
    \caption{Comparison of the Outputted BPs in Sample}
    \label{fig:bp}
\end{figure}

\begin{mdframed}
\begin{remark}(Bonus Material)
A careful asset manager in the bank, given the \textit{breach probability} formula that we derived can work even further to minimize the risk of potential exposures. As we mentioned in Remark \ref{weights} we can model the portfolio weights not to be constant meaning that we can assume that trading is allowed during the lifespan of the loan (which is what happens most of the times in real world). Hence, she can try to minimize the \textit{breach probability} by recalibrating the portfolio at every time step. To put things mathematically the problem that she is asked to solve is an optimization one given by:
\[\mathbf{w}^*_t\coloneqq\underset{\mathbf{w}_t\in\mathfrak{W}}{\arg\min\text{  }}BP_t(\mathbf{w}_t)\]
where, $\mathfrak{W}$ is a specified subspace of $\mathbb{R}^d$.

Then, by exploiting the monotonicity of $\Phi(\cdot)$ and the positive definiteness of $\Sigma_{t+1}$ the optimization problem blows up to:
\[\mathbf{w}^*_t\coloneqq\underset{\mathbf{w}_t\in\mathfrak{W}}{\arg\min\text{  }}\frac{L_{t+1}-\mathbf{w}^\intercal D^\mathbf{S}_t\mathbf{e_d}}{\mathbf{w}^\intercal D^\mathbf{S}_t\Sigma_{t+1} (D^\mathbf{S}_t)^\intercal \mathbf{w}}\]

which is equivalent to:
\[\mathbf{w}^*_t\coloneqq\underset{\mathbf{w}_t\in\mathfrak{W}}{\arg\max\text{  }}\frac{\mathbf{w}^\intercal D^\mathbf{S}_t\mathbf{e_d}}{\mathbf{w}^\intercal D^\mathbf{S}_t\Sigma_{t+1} (D^\mathbf{S}_t)^\intercal \mathbf{w}}+\lambda \left(\mathbf{w}^\intercal D^\mathbf{S}_t\Sigma_{t+1} (D^\mathbf{S}_t)^\intercal \mathbf{w}\right)\]

Intuitively, the optimization problem is analogous to maximizing the Sharpe Ration of the portfolio with constraints on its variance in order to be above a specific threshold dependent on $\lambda = 1/L_{t+1}\in \mathbb{R}$.
\end{remark}
\end{mdframed}
\subsection{Forecasted Breach Probability
}

In order to provide estimates for the forecasted \textit{breach probability}, we should first focus on the forecast of a DCC model.

According to \cite{DCC_Forecast}, the multi-step ahead forecasting of a DCC model takes place in two stages. At first, the individual forecasted volatilities of each asset need to me calculated and then the forecasted corellation matrices which will result to the forecasted covariance matrices. 

\subsubsection{Multi-Step Ahead Forecast of GARCH(1,1)}\label{garchFor}
By exploiting the simple recursive relationship for the conditional volatility as described in \ref{thm:garch_def} (for $p=q=1$), we have that for $l=2,3, \dots$
\begin{equation}
\begin{split}
    h_t^2(l)\coloneqq\mathbb{E}[\sigma^2_{t+l}|\mathcal{F}_t]=\omega+(\alpha+\beta)h_t(l-1)\\
    =\frac{\omega[1-(\alpha+\beta)^{l-1}]}{1-\alpha-\beta}+(\alpha+\beta)^{l-1}\sigma^2_{t+1}
\end{split}
\end{equation}
\subsubsection{Multi-Step Ahead Forecast of LSTM}\label{lstmFor}
In that case, the complicated structure of a LSTM layer as described in \ref{lstm} does not allow us to find a similar simple solution like in the GARCH(1,1) case. Hence, we can calculate the forecasted conditional probability by bootstraping. The detailed process can be described in the following algorithm.
\begin{algorithm}[H]
\caption{Bootstraping Conditional Volatility}
\begin{algorithmic}
\State{
\begin{enumerate}
    \item Specify \# of simulated paths $M$ and forecast horizon $H$.
    \item For $i = 1, \dots, M$:
    \begin{enumerate}
        \item $h_t^i(1)\gets\hat{\sigma}^{\text{LSTM}}(\mathbf{x}_t|\boldsymbol{\theta}^*)$ ($\mathbf{x}_t$ the vector as described in \ref{dataContr})
        \item For $l=1, \dots, H$:
        \begin{enumerate}
            \item Sample $\varepsilon\sim\mathcal{N}(0,1)$
            \item $r_{t+l}^i\gets h_t^i(l)\varepsilon$
            \item $h_t^i(l+1)\gets \hat{\sigma}^{\text{LSTM}}(\mathbf{x}_{t+l}^i|\boldsymbol{\theta}^*)$
        \end{enumerate}
    \end{enumerate}
    \item Estimate $h_t(l) = \frac{1}{M}\sum_{i=1}^M h_t^i(l)$ for $l=1, \dots, H$
\end{enumerate}
}
\end{algorithmic}
\end{algorithm}

\subsubsection{Multi-Step Ahead Forecast of DCC}\label{dccFor}

As mentioned, in order to forecast the DCC-GARCH/LSTM model for a future horizon of $H$ (days), first we have to forecast the conditional corellation matrices and afterwards from the individual forecasted volatilities we can obtain the forecasted conditional covariance matrices.

For the conditional correlation matrix forecast two methods were proposed in \cite{DCC_Forecast}, but we pick specifically the one of them as empirical analysis in the specific paper show that is has better bias properties for almost every correlation matrix.

To do that, the main assumption used is

 \[\mathbb{E}[P_{t+i}|\mathcal{F}_t]=\mathbb{E}[Q_{t+i}|\mathcal{F}_t] \text{ ,for all } i=1, \dots, H\]. Therefore one has,
\begin{equation}
    \hat{P}_t(1)\coloneqq\mathbb{E}[P_{t+1}|\mathcal{F}_{t}]\approx\mathbb{E}[Q_{t+1}|\mathcal{F}_t]=(1-\alpha-\beta)P_c+\alpha Y_t Y_t^\intercal+\beta Q_t
\end{equation}
and for $l = 1, \dots, H$
\begin{equation}
    \begin{split}
        \hat{P}_t(l)\coloneqq\mathbb{E}[P_{t+k}|\mathcal{F}_t]\approx\mathbb{E}[Q_{t+k}|\mathcal{F}_t]\approx (1-\alpha-\beta)P_c+(\alpha+\beta)\hat{P}_t(l-1)\\\approx  \dots \\ \approx (1-(\alpha+\beta)^{k-1})P_c+(\alpha+\beta)^{k-1}\hat{P}_t(1)
    \end{split}
\end{equation}
Now for the forecasted conditional correlation matrices $\hat{\Sigma}_t(l)\coloneqq\mathbb{E}[\Sigma_{t+l}|\mathcal{F}_{t}]$ one has
\begin{equation}
    \hat{\Sigma}_t(l) = \hat{\Delta}_t(l)\hat{P}_t(l)\hat{\Delta}_t(l)\text{ , for } l = 1, \dots, H
\end{equation}
where, $\hat{\Delta}_t(l)\coloneqq\text{diag}(h^1_t(l),\dots, h^d_t(l))\in\mathbb{R}^{d\times d}$

\bigskip\bigskip
To what follows now, after the described forecasting methodologies in \ref{garchFor}, \ref{lstmFor} and \ref{dccFor}, we are able to proceed with the calculation of the forecasted breach probability. For that, due to the fact that during the whole modelling the change in level of returns is considered insignificant ($\mathbb{E}[\mathbf{R}_{t+1}|\mathcal{F}_t]=0$) and also we focus in forecasting horizons of relative small size ($H\leq 5$) we make the assumption that
\[\mathbf{S}_{t+l}|\mathcal{F}_t \overset{d}{=}\mathbf{S}_t\odot\left(\mathbf{e_d}+\mathbf{R}_{t+l}|\mathcal{F}_t\right) \text{ , for } l=2,\dots, H\]
admittedly, we assume that the liabilities amount remains the same during the forecasting horizon
\[L_{t+l} = L_{t+1}\text{ , for } l=2,\dots, H\]. Consequently, we can now forecast the $l$-step ahead \textit{breach probability} similar to \ref{BP} as follows
\begin{equation}
    \begin{split}
        \widehat{BP}_t(l) = \mathbb{P}(V_{t+l}< L_{t+l}|\mathcal{F}_t) = \mathbb{P}\left(\mathbf{w}^\intercal\mathbf{S}_{t+l}< L_{t+1}|\mathcal{F}_t\right) \\
        = \mathbb{P}(\mathbf{w}^\intercal[\mathbf{S}_t\odot(\mathbf{e_d}+\mathbf{R}_{t+l})]< L_{t+1}|\mathcal{F}_t)\\
        = \mathbb{P}\left(\mathbf{w}^\intercal \left[ D_t^\mathbf{S}(\mathbf{e_d}+\mathbf{R}_{t+l})\right]<L_{t+1}|\mathcal{F}_t\right)\\
        =\mathbb{P}\left(\mathbf{w}^\intercal\left( D^\mathbf{S}_t\mathbf{e_d}\right)+\mathbf{w}^\intercal D^\mathbf{S}_t\hat{\Sigma}_{t}(l) (D^\mathbf{S}_t)^\intercal \mathbf{w} Z<L_{t+1}| \mathcal{F}_t\right)\\
=\Phi\left(\frac{L_{t+1}-\mathbf{w}^\intercal D^\mathbf{S}_t\mathbf{e_d}}{\mathbf{w}^\intercal D^\mathbf{S}_t\hat{\Sigma}_{t}(l) (D^\mathbf{S}_t)^\intercal \mathbf{w}}\right)
    \end{split}
\end{equation}

We therefore, forecast the \textit{breach probability} on 11/03/2020 for a horizon of 5 days. The outputted forecasts can be found in Figure \ref{fig:fbp}.

One can notice that the DCC-LSTM model tends to estimate higher conditional volatilities compared to the DCC-GARCH, which results to higher \textit{breach probabilities}.

\begin{figure}[H]
    \centering
    \includegraphics[width = 12cm]{figures/fbp.png}
    \caption{Comparison of the Forecasted BP}
    \label{fig:fbp}
\end{figure}

\section{Lending Value Application}\label{lendingValue}

The estimation of the \textit{lending value} is a very important operation for the pricing of a Lombard loan. The literature research on contrary to the mathematical elegance of such an application is rather poor. Though, the paper \cite{juri} describes a rigorous mathematical modelling of such a loan and derives closed form estimations of the \textit{lending value} with and without liquidity constraints. On the other hand, the modelling is restricted only to one-dimensional collateral with their stochastic dynamics described by Geometric Brownian Motion, assumptions which are quite strong. 

During the specific subsection, we aim to develop a method for the estimation of the \textit{lending value} for portfolios of collateral with stochastic dynamics from the DCC model. Furthermore, we assume that trading is not allowed during the life of the portfolio and therefore its weights remain constant until maturity.

For the shake of economy, we use the same notation as in \ref{signal} with the difference that now the liabilities amount is not abstractly determined but instead is given from
\begin{equation}
    L_{t+1} \coloneqq \lambda V_t \text{ , for } t=0, \dots, T-1
\end{equation}
where with $\lambda$ we denote the \textit{lending value}.

Hence, the bank needs to cover potential exposures in the case where the the borrower does not react to the margin calls. By, denoting the \textit{closeout period} with $
\delta$, the bank can estimate the \textit{lending value} by trying to keep the probability "that the collateral's value after the \textit{closeout period} has dropped below the outstanding amount" low. By assuming that the outstanding amount during the \textit{closeout period} remains the same as it was at the margin call's timestep one requires that for a small $\epsilon\in(0,1)$
\begin{equation}
    \mathbb{P}(V_{t+\delta}<\lambda V_t)<\epsilon\text{ , for }t = 1, \dots, T-\delta
\end{equation}

The problem that occurs here, is that on contrary to the the application in \ref{signal} the stated probabilities are unconditional. More specifically, the DCC model does not have always a stationary distribution and it does only under specific cases with complex solutions \cite{dccStationarity}. 

To deal with that issue, we propose the following methodology based on Monte-Carlo simulations described in detail in Algorithm \ref{alg:lamdaEst}.


\begin{algorithm} [h]
\caption{Estimation of $\lambda$}
\label{alg:lamdaEst}
\begin{algorithmic}
\State{ \textbf{Input:} Portfolio of $d$ assets with the dataset of historical prices $\mathcal{S} = (\mathbf{S}_\tau)_{\tau = 1}^{t_0}$ and the portfolio weights $\mathbf{w}\in\mathbb{R}^d$. \\\hrulefill\smallskip
\begin{enumerate}
\item Specify forecasting horizon $H$ and \# of simulations $M$.
\item Fit a DCC model on the historical portfolio returns.
\item Simulate $M$ paths from the fitted DCC model for the $d$-dimensional returns and the future horizon $H$.
\[\mathcal{R}^i = \left[\mathbf{R}^{i}_{t_1},\dots, \mathbf{R}^{i}_{t_H}\right]\text{ , for }i=1, \dots, M\]
\[\mathbf{R}^{i}_{t_h} = (r^{i,1}_{t_h}, \dots, r^{i,d}_{t_h})^\intercal\in\mathbb{R}^d\text{ , for } i=1, \dots, M\text{ and } h = 1, \dots, H\]
\item Calculate the portfolio value paths.
\[\mathbf{V}^i = \left(V^i_{t_1}, \dots, V^i_{t_H}\right)^\intercal\in \mathbb{R}_+^H\text{ , for }i=1, \dots, M\]
\[V^i_{t_h} = \mathbf{w}^\intercal \left(\mathbf{S}_{t_0}\odot \left[\underset{k=1}{\overset{h}{\odot}}\left(\mathbf{e_d}+\mathbf{R}^i_{t_k}
\right)\right]\right)\text{ , for } i=1, \dots, M\text{ and } h = 1, \dots, H\]
\item Select a closeout period $\delta$.
\item Create a dataset $\mathcal{P}\coloneqq\left(p^i_{t_{l+\delta}}\right)_{l=0}^{H-\delta}$ where 
\[p^i_{t_{l+\delta}}\coloneqq\frac{V^i_{t_{l+\delta}}-V^i_{t_l}}{V^i_{t_l}}\text{ , for } i=1, \dots, M \text{ and } l=0, \dots, H-\delta\]
\item Estimate the empirical distribution funciton (e.d.f.) $\hat{F}(\cdot)$ of all the data in $\mathcal{P}$.
\item Estimate the inverse of e.d.f $\hat{F}^{-1}(\cdot)$ from linear interpolation.
\end{enumerate}
}
\hrulefill\smallskip
\State {Now, one has,
\[\epsilon>\mathbb{P}\left(V_{t+\delta}<\lambda V_t\right) =  \mathbb{P}\left(\frac{V_{t+\delta}-V_t}{V_t}<\lambda-1\right)\approx\hat{F}(\lambda-1)\]
\[\Leftrightarrow \]
\[\lambda<\hat{F}^{-1}(\epsilon)+1\]
}
\hrulefill\smallskip
\State {\textbf{Output:} Estimate $\lambda$ via its derived upper bound $\hat{F}^{-1}(\epsilon)+1$.}
\end{algorithmic}
\end{algorithm}


\begin{figure}[h]
    \centering
    \includegraphics[width = 13cm]{figures/ll_lines_garch.png}
    \caption{\textit{Lending Value} $\lambda$ from the DCC-GARCH Model}
    \label{fig:garch_lines}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width = 13cm]{figures/ll_lines_lstm.png}
    \caption{\textit{Lending Value} $\lambda$ from the DCC-LSTM Model}
    \label{fig:lstm_lines}
\end{figure}

\begin{figure}[h]
    \centering
    \subfloat[\centering GARCH]{{\includegraphics[width=6cm]{figures/garch_ll.png} }}
    \qquad
    \subfloat[\centering LSTM]{{\includegraphics[width=6cm]{figures/lstm_ll.png} }}
    \caption{\textit{Lending Value} $\lambda$ Surface}
    \label{fig:surface_LL}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width = 12cm]{figures/QQ_simuls.png}
    \caption{Quantiles Comparison}
    \label{QQ_simuls}
\end{figure}

We apply the proposed algorithm in the full dataset \ref{ll_portf} of the portfolio used for the application in \ref{signal}. Regarding the simulation, we picked a forecasting horizon $H=255$ for a total of $M = 10^4$ number of paths. The estimated \textit{lending values} for different values of $\epsilon$ and \textit{closeout periods} can be illustrated in Figures \ref{fig:garch_lines} and \ref{fig:lstm_lines}. Moreover, by interpolating we can visualize the \textit{lending value surfaces} in Figure \ref{fig:surface_LL}.

We can notice that, the estimated \textit{lending value} from the DCC-LSTM model is more conservative, as the LSTM models tend to estimate higher conditional volatilities than the GARCH ones. Moreover, it is clear that the cost of lending increase as the \textit{closeout period} increases.

Last but not least, we evaluate the "reliability" of the simulations by comparing the quantiles of the initial dataset with the ones simulated from the DCC-GARCH and DCC-LSTM respectively. The comparison can be found in the QQ-plot of Figure \ref{QQ_simuls}. Due to the very volatile market period in which we examined the portfolio, both of the simulations underestimate the tails of the returns' distribution. Although, the DCC-LSTM seems to be way more accurate. The similarity between the DCC-LSTM simulation can be furtherly illustrated in the kernel density estimations in Figure \ref{fig:kde} from the Appendix.
%--------------------------------------------------------
\chapter{Appendix}
\section{Data}
\subsection{One-Dimensional Data} \label{ssec:1d_data}
The dataset for the analysis of the one dimensional returns was obtained from the \textit{Bloomberg L.P.} software with access from \textit{Zanders Treasury \& Risk}.

We investigated 8 different stock indices with acronyms as follows:
\begin{itemize}
\item GSPC: S\&P 500 INDEX
\item DJI: Dow Jones Industrial Average
\item IXIC: NASDAQ Composite
\item RUT: Russel 2000
\item SSMI: SMI PR
\item OEX: S\&P 100 INDEX
\item N225: NIkkei 225
\item FTSE: FTSE 100
\end{itemize}

The summary statistics of every index alongside with the starting and ending date of investigation are provided in table \ref{tab:summary_1d}.

Admittedly, the values, histograms and autocorrelation plots of the returns can be illustrated in the figures \ref{fig:1d_rets}, \ref{fig:1d_hists} and \ref{fig:1d_auto} respectively.

For most of the returns, the value of the previous day (lag $=$ 1) seems to correlates with the current value and other less significant autocorrelations appear on the 16th previous day (lag $=$ 16). Features that agree with the modelling assumption of conditionally distributed returns.

Moreover, volatility clusters vividly appear on severe periods of the market such us the 2008 crisis and the COVID-19 outbreak. 

Last but not least, the sample distributions of the different indices seem to differ significantly, as fatter tails are observed in IXIC, RUT and N225, fact which contributes to the robustness in the performance of the models.

Regarding the stationarity of the returns we performed the Augmented-Dickey Fuller test \cite{ADF} for each of the series. The resulted test-statistics for every test can be demonstrated in \ref{tb:ADF_table} are significantly smaller than the critical values, concluding to reject the null hypothesis that the time series are non-stationary.

\begin{table}[!ht]\label{tb:ADF_table}
    \scriptsize
    \centering
    \begin{tabular}{|p{1cm}||p{1.cm}|p{1.cm}|p{1.cm}|p{1.cm}|p{1.cm}|p{1.cm}|p{1.cm}|p{1.cm}|}
    \hline
        ~ & GSPC & DJI & IXIC & RUT & SSMI & OEX & N225 & FTSE \\ \hline\hline
        Test Statistic & -14.205 & -14.838 & -13.503 & -13.904 & -13.532 & -14.255 & -12.891 & -14.663 \\ \hline
        p-value & 1.76e-26&	1.85e-27&	2.95e-25&	5.66e-26&	2.61e-25&	1.46e-26&	4.45e-24&	3.36e-27 \\ \hline
        No. lags & 33 & 33 & 33 & 33 & 33 & 33 & 33 & 33 \\ \hline
        No. observations & 5608 & 5608 & 5608 & 5608 & 5595 & 5608 & 5458 & 5629 \\ \hline
        1\% Critical Value & -3.432 & -3.432 & -3.432 & -3.432 & -3.432 & -3.432 & -3.432 & -3.432 \\ \hline
        5\% Critical Value & -2.862 & -2.862 & -2.862 & -2.862 & -2.862 & -2.862 & -2.862 & -2.862 \\ \hline
        10\% Critical Value & -2.567 & -2.567 & -2.567 & -2.567 & -2.567 & -2.567 & -2.567 & -2.567 \\ \hline
    \end{tabular}
    \normalsize	
    \caption{Augmented Dickey–Fuller tests}
\end{table}


\begin{figure}\
    \centering
    \includegraphics[width = 13cm]{figures/RETURNS.png}
    \caption{Percentage Returns}
    \label{fig:1d_rets}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width = 13cm]{figures/HISTOGRAMMS.png}
    \caption{Histograms of Percentage Returns}
    \label{fig:1d_hists}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width = 13cm]{figures/AUTOCORRELATIONS.png}
    \caption{Auto-correlation of Percentage Returns}
    \label{fig:1d_auto}
\end{figure}

\begin{table}[!ht] \label{tab:summary_1d}
    \centering
    \scriptsize
    \begin{tabular}{|p{1.2cm}||p{1.cm}|p{1.cm}|p{1.cm}|p{1.cm}|p{1.cm}|p{1.cm}|p{1.cm}|p{1.cm}|}
    \hline
        ~ & GSPC & DJI & IXIC & RUT & SSMI & OEX & N225 & FTSE \\ \hline\hline
        1st Day & 3/1/00 & 3/1/00 & 3/1/00 & 3/1/00 & 5/1/00 & 3/1/00 & 5/1/00 & 5/1/00 \\ \hline
        Last Day & 3/6/22 & 3/6/22 & 3/6/22 & 3/6/22 & 3/6/22 & 3/6/22 & 3/6/22 & 1/6/22 \\ \hline
        Count & 5642 & 5642 & 5642 & 5642 & 5629 & 5642 & 5491 & 5662 \\ \hline
        Mean & 0.03 & 0.03 & 0.03 & 0.04 & 0.01 & 0.02 & 0.02 & 0.01 \\ \hline
        Std & 1.24 & 1.19 & 1.59 & 1.56 & 1.14 & 1.24 & 1.47 & 1.18 \\ \hline
        Min & -11.98 & -12.93 & -12.32 & -14.27 & -9.64 & -11.57 & -11.41 & -10.87 \\ \hline
        5\%  & -1.90 & -1.80 & -2.60 & -2.39 & -1.76 & -1.90 & -2.30 & -1.83 \\ \hline
        50\%  & 0.06 & 0.05 & 0.09 & 0.08 & 0.05 & 0.06 & 0.04 & 0.05 \\ \hline
        95\%  & 1.74 & 1.68 & 2.33 & 2.33 & 1.65 & 1.76 & 2.20 & 1.76 \\ \hline
        Max & 11.58 & 11.37 & 14.17 & 9.39 & 11.39 & 11.24 & 14.15 & 9.84 \\ \hline
    \end{tabular}
    \normalsize
    \caption{Summary Statistics for the Percentage Returns}
\end{table}

\begin{figure}

\begin{multicols}{2}
{\fontsize{2.5}{4}\selectfont

\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:} &        GSPC        & \textbf{  R-squared:         } &     0.000   \\
\textbf{Mean Model:}    &   Constant Mean    & \textbf{  Adj. R-squared:    } &     0.000   \\
\textbf{Vol Model:}     &       GARCH        & \textbf{  Log-Likelihood:    } &   -5634.73  \\
\textbf{Distribution:}  &       Normal       & \textbf{  AIC:               } &    11277.5  \\
\textbf{Method:}        & Maximum Likelihood & \textbf{  BIC:               } &    11302.6  \\
\textbf{}               &                    & \textbf{  No. Observations:  } &    3949     \\
\textbf{Date:}          &  Mon, Jun 06 2022  & \textbf{  Df Residuals:      } &    3948     \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{mu} &       0.0356  &    1.351e-02     &     2.637  &      8.369e-03       &   [9.146e-03,6.211e-02]     \\
                  & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{omega}    &       0.0178  &    4.824e-03     &     3.684  &      2.293e-04       &   [8.319e-03,2.723e-02]     \\
\textbf{alpha[1]} &       0.0968  &    1.221e-02     &     7.925  &      2.282e-15       &    [7.283e-02,  0.121]      \\
\textbf{beta[1]}  &       0.8907  &    1.275e-02     &    69.869  &        0.000         &     [  0.866,  0.916]       \\
\bottomrule
\end{tabular}
%\caption{Constant Mean - GARCH Model Results}
\end{center}

  
\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:} &        GSPC        & \textbf{  R-squared:         } &     0.000   \\
\textbf{Mean Model:}    &   Constant Mean    & \textbf{  Adj. R-squared:    } &     0.000   \\
\textbf{Vol Model:}     &     GJR-GARCH      & \textbf{  Log-Likelihood:    } &   -5538.65  \\
\textbf{Distribution:}  &       Normal       & \textbf{  AIC:               } &    11087.3  \\
\textbf{Method:}        & Maximum Likelihood & \textbf{  BIC:               } &    11118.7  \\
\textbf{}               &                    & \textbf{  No. Observations:  } &    3949     \\
\textbf{Date:}          &  Mon, Jun 06 2022  & \textbf{  Df Residuals:      } &    3948     \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{mu} &  -7.1443e-03  &    1.389e-02     &    -0.514  &          0.607       &   [-3.437e-02,2.008e-02]    \\
                  & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{omega}    &       0.0190  &    4.450e-03     &     4.278  &      1.887e-05       &   [1.031e-02,2.776e-02]     \\
\textbf{alpha[1]} &   2.5982e-10  &    1.225e-02     & 2.122e-08  &          1.000       &   [-2.400e-02,2.400e-02]    \\
\textbf{gamma[1]} &       0.1731  &    2.175e-02     &     7.958  &      1.743e-15       &     [  0.130,  0.216]       \\
\textbf{beta[1]}  &       0.8987  &    1.619e-02     &    55.493  &        0.000         &     [  0.867,  0.930]       \\
\bottomrule
\end{tabular}
%\caption{Constant Mean - GJR-GARCH Model Results}
\end{center}

  
\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:} &        GSPC        & \textbf{  R-squared:         } &     0.000   \\
\textbf{Mean Model:}    &   Constant Mean    & \textbf{  Adj. R-squared:    } &     0.000   \\
\textbf{Vol Model:}     &       EGARCH       & \textbf{  Log-Likelihood:    } &   -5535.30  \\
\textbf{Distribution:}  &       Normal       & \textbf{  AIC:               } &    11080.6  \\
\textbf{Method:}        & Maximum Likelihood & \textbf{  BIC:               } &    11112.0  \\
\textbf{}               &                    & \textbf{  No. Observations:  } &    3949     \\
\textbf{Date:}          &  Mon, Jun 06 2022  & \textbf{  Df Residuals:      } &    3948     \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{mu} &  -6.0538e-03  &    1.333e-02     &    -0.454  &          0.650       &   [-3.218e-02,2.007e-02]    \\
                  & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{omega}    &   2.2315e-03  &    3.045e-03     &     0.733  &          0.464       &   [-3.736e-03,8.199e-03]    \\
\textbf{alpha[1]} &       0.1159  &    1.600e-02     &     7.244  &      4.359e-13       &    [8.454e-02,  0.147]      \\
\textbf{gamma[1]} &      -0.1478  &    1.491e-02     &    -9.911  &      3.740e-23       &     [ -0.177, -0.119]       \\
\textbf{beta[1]}  &       0.9793  &    4.376e-03     &   223.780  &        0.000         &     [  0.971,  0.988]       \\
\bottomrule
\end{tabular}
%\caption{Constant Mean - EGARCH Model Results}
\end{center}

  
\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:} &         DJI        & \textbf{  R-squared:         } &     0.000   \\
\textbf{Mean Model:}    &   Constant Mean    & \textbf{  Adj. R-squared:    } &     0.000   \\
\textbf{Vol Model:}     &       GARCH        & \textbf{  Log-Likelihood:    } &   -5415.23  \\
\textbf{Distribution:}  &       Normal       & \textbf{  AIC:               } &    10838.5  \\
\textbf{Method:}        & Maximum Likelihood & \textbf{  BIC:               } &    10863.6  \\
\textbf{}               &                    & \textbf{  No. Observations:  } &    3949     \\
\textbf{Date:}          &  Mon, Jun 06 2022  & \textbf{  Df Residuals:      } &    3948     \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{mu} &       0.0353  &    1.289e-02     &     2.741  &      6.121e-03       &   [1.007e-02,6.061e-02]     \\
                  & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{omega}    &       0.0168  &    4.380e-03     &     3.841  &      1.227e-04       &   [8.238e-03,2.541e-02]     \\
\textbf{alpha[1]} &       0.1028  &    1.328e-02     &     7.741  &      9.882e-15       &    [7.679e-02,  0.129]      \\
\textbf{beta[1]}  &       0.8847  &    1.326e-02     &    66.717  &        0.000         &     [  0.859,  0.911]       \\
\bottomrule
\end{tabular}
%\caption{Constant Mean - GARCH Model Results}
\end{center}

  
\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:} &         DJI        & \textbf{  R-squared:         } &     0.000   \\
\textbf{Mean Model:}    &   Constant Mean    & \textbf{  Adj. R-squared:    } &     0.000   \\
\textbf{Vol Model:}     &     GJR-GARCH      & \textbf{  Log-Likelihood:    } &   -5326.13  \\
\textbf{Distribution:}  &       Normal       & \textbf{  AIC:               } &    10662.3  \\
\textbf{Method:}        & Maximum Likelihood & \textbf{  BIC:               } &    10693.7  \\
\textbf{}               &                    & \textbf{  No. Observations:  } &    3949     \\
\textbf{Date:}          &  Mon, Jun 06 2022  & \textbf{  Df Residuals:      } &    3948     \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{mu} &  -4.2380e-03  &    1.301e-02     &    -0.326  &          0.745       &   [-2.975e-02,2.127e-02]    \\
                  & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{omega}    &       0.0164  &    3.735e-03     &     4.384  &      1.163e-05       &   [9.055e-03,2.369e-02]     \\
\textbf{alpha[1]} &   1.8615e-09  &    1.062e-02     & 1.753e-07  &          1.000       &   [-2.081e-02,2.081e-02]    \\
\textbf{gamma[1]} &       0.1756  &    2.299e-02     &     7.640  &      2.164e-14       &     [  0.131,  0.221]       \\
\textbf{beta[1]}  &       0.8993  &    1.469e-02     &    61.237  &        0.000         &     [  0.870,  0.928]       \\
\bottomrule
\end{tabular}
%\caption{Constant Mean - GJR-GARCH Model Results}
\end{center}

  
\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:} &         DJI        & \textbf{  R-squared:         } &     0.000   \\
\textbf{Mean Model:}    &   Constant Mean    & \textbf{  Adj. R-squared:    } &     0.000   \\
\textbf{Vol Model:}     &       EGARCH       & \textbf{  Log-Likelihood:    } &   -5322.12  \\
\textbf{Distribution:}  &       Normal       & \textbf{  AIC:               } &    10654.2  \\
\textbf{Method:}        & Maximum Likelihood & \textbf{  BIC:               } &    10685.6  \\
\textbf{}               &                    & \textbf{  No. Observations:  } &    3949     \\
\textbf{Date:}          &  Mon, Jun 06 2022  & \textbf{  Df Residuals:      } &    3948     \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{mu} &  -4.9676e-03  &    1.260e-02     &    -0.394  &          0.693       &   [-2.967e-02,1.973e-02]    \\
                  & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{omega}    &   4.3778e-04  &    2.899e-03     &     0.151  &          0.880       &   [-5.244e-03,6.119e-03]    \\
\textbf{alpha[1]} &       0.1247  &    1.717e-02     &     7.264  &      3.765e-13       &    [9.107e-02,  0.158]      \\
\textbf{gamma[1]} &      -0.1421  &    1.464e-02     &    -9.707  &      2.823e-22       &     [ -0.171, -0.113]       \\
\textbf{beta[1]}  &       0.9791  &    4.254e-03     &   230.143  &        0.000         &     [  0.971,  0.987]       \\
\bottomrule
\end{tabular}
%\caption{Constant Mean - EGARCH Model Results}
\end{center}

  
\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:} &        IXIC        & \textbf{  R-squared:         } &     0.000   \\
\textbf{Mean Model:}    &   Constant Mean    & \textbf{  Adj. R-squared:    } &     0.000   \\
\textbf{Vol Model:}     &       GARCH        & \textbf{  Log-Likelihood:    } &   -6658.56  \\
\textbf{Distribution:}  &       Normal       & \textbf{  AIC:               } &    13325.1  \\
\textbf{Method:}        & Maximum Likelihood & \textbf{  BIC:               } &    13350.2  \\
\textbf{}               &                    & \textbf{  No. Observations:  } &    3949     \\
\textbf{Date:}          &  Mon, Jun 06 2022  & \textbf{  Df Residuals:      } &    3948     \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{mu} &       0.0526  &    1.751e-02     &     3.006  &      2.648e-03       &   [1.831e-02,8.694e-02]     \\
                  & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{omega}    &       0.0185  &    4.759e-03     &     3.886  &      1.020e-04       &   [9.165e-03,2.782e-02]     \\
\textbf{alpha[1]} &       0.0828  &    1.060e-02     &     7.814  &      5.559e-15       &    [6.205e-02,  0.104]      \\
\textbf{beta[1]}  &       0.9091  &    1.062e-02     &    85.602  &        0.000         &     [  0.888,  0.930]       \\
\bottomrule
\end{tabular}
%\caption{Constant Mean - GARCH Model Results}
\end{center}

  
\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:} &        IXIC        & \textbf{  R-squared:         } &     0.000   \\
\textbf{Mean Model:}    &   Constant Mean    & \textbf{  Adj. R-squared:    } &     0.000   \\
\textbf{Vol Model:}     &     GJR-GARCH      & \textbf{  Log-Likelihood:    } &   -6593.88  \\
\textbf{Distribution:}  &       Normal       & \textbf{  AIC:               } &    13197.8  \\
\textbf{Method:}        & Maximum Likelihood & \textbf{  BIC:               } &    13229.2  \\
\textbf{}               &                    & \textbf{  No. Observations:  } &    3949     \\
\textbf{Date:}          &  Mon, Jun 06 2022  & \textbf{  Df Residuals:      } &    3948     \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{mu} &   8.6125e-03  &    1.721e-02     &     0.500  &          0.617       &   [-2.512e-02,4.235e-02]    \\
                  & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{omega}    &       0.0191  &    4.967e-03     &     3.855  &      1.158e-04       &   [9.411e-03,2.888e-02]     \\
\textbf{alpha[1]} &   4.1946e-17  &    7.526e-03     & 5.574e-15  &          1.000       &   [-1.475e-02,1.475e-02]    \\
\textbf{gamma[1]} &       0.1340  &    1.816e-02     &     7.378  &      1.606e-13       &    [9.842e-02,  0.170]      \\
\textbf{beta[1]}  &       0.9228  &    1.231e-02     &    74.985  &        0.000         &     [  0.899,  0.947]       \\
\bottomrule
\end{tabular}
%\caption{Constant Mean - GJR-GARCH Model Results}
\end{center}

  
\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:} &        IXIC        & \textbf{  R-squared:         } &     0.000   \\
\textbf{Mean Model:}    &   Constant Mean    & \textbf{  Adj. R-squared:    } &     0.000   \\
\textbf{Vol Model:}     &       EGARCH       & \textbf{  Log-Likelihood:    } &   -6591.56  \\
\textbf{Distribution:}  &       Normal       & \textbf{  AIC:               } &    13193.1  \\
\textbf{Method:}        & Maximum Likelihood & \textbf{  BIC:               } &    13224.5  \\
\textbf{}               &                    & \textbf{  No. Observations:  } &    3949     \\
\textbf{Date:}          &  Mon, Jun 06 2022  & \textbf{  Df Residuals:      } &    3948     \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{mu} &   4.6614e-03  &    1.705e-02     &     0.273  &          0.785       &   [-2.876e-02,3.809e-02]    \\
                  & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{omega}    &   8.6029e-03  &    3.203e-03     &     2.686  &      7.231e-03       &   [2.325e-03,1.488e-02]     \\
\textbf{alpha[1]} &       0.1099  &    1.642e-02     &     6.693  &      2.187e-11       &    [7.774e-02,  0.142]      \\
\textbf{gamma[1]} &      -0.1062  &    1.097e-02     &    -9.675  &      3.846e-22       &    [ -0.128,-8.465e-02]     \\
\textbf{beta[1]}  &       0.9863  &    2.852e-03     &   345.832  &        0.000         &     [  0.981,  0.992]       \\
\bottomrule
\end{tabular}
%\caption{Constant Mean - EGARCH Model Results}
\end{center}

  
\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:} &         RUT        & \textbf{  R-squared:         } &     0.000   \\
\textbf{Mean Model:}    &   Constant Mean    & \textbf{  Adj. R-squared:    } &     0.000   \\
\textbf{Vol Model:}     &       GARCH        & \textbf{  Log-Likelihood:    } &   -6743.16  \\
\textbf{Distribution:}  &       Normal       & \textbf{  AIC:               } &    13494.3  \\
\textbf{Method:}        & Maximum Likelihood & \textbf{  BIC:               } &    13519.4  \\
\textbf{}               &                    & \textbf{  No. Observations:  } &    3949     \\
\textbf{Date:}          &  Mon, Jun 06 2022  & \textbf{  Df Residuals:      } &    3948     \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{mu} &       0.0337  &    1.883e-02     &     1.788  &      7.380e-02       &   [-3.241e-03,7.057e-02]    \\
                  & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{omega}    &       0.0398  &    9.273e-03     &     4.293  &      1.760e-05       &   [2.164e-02,5.798e-02]     \\
\textbf{alpha[1]} &       0.0870  &    1.101e-02     &     7.904  &      2.697e-15       &    [6.546e-02,  0.109]      \\
\textbf{beta[1]}  &       0.8932  &    1.289e-02     &    69.302  &        0.000         &     [  0.868,  0.918]       \\
\bottomrule
\end{tabular}
%\caption{Constant Mean - GARCH Model Results}
\end{center}

  
\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:} &         RUT        & \textbf{  R-squared:         } &     0.000   \\
\textbf{Mean Model:}    &   Constant Mean    & \textbf{  Adj. R-squared:    } &     0.000   \\
\textbf{Vol Model:}     &     GJR-GARCH      & \textbf{  Log-Likelihood:    } &   -6686.91  \\
\textbf{Distribution:}  &       Normal       & \textbf{  AIC:               } &    13383.8  \\
\textbf{Method:}        & Maximum Likelihood & \textbf{  BIC:               } &    13415.2  \\
\textbf{}               &                    & \textbf{  No. Observations:  } &    3949     \\
\textbf{Date:}          &  Mon, Jun 06 2022  & \textbf{  Df Residuals:      } &    3948     \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{mu} &      -0.0143  &    1.912e-02     &    -0.748  &          0.455       &   [-5.177e-02,2.318e-02]    \\
                  & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{omega}    &       0.0415  &    9.592e-03     &     4.324  &      1.530e-05       &   [2.268e-02,6.028e-02]     \\
\textbf{alpha[1]} &     0.0000    &    7.306e-03     &   0.000    &          1.000       &   [-1.432e-02,1.432e-02]    \\
\textbf{gamma[1]} &       0.1436  &    2.046e-02     &     7.018  &      2.251e-12       &     [  0.104,  0.184]       \\
\textbf{beta[1]}  &       0.9069  &    1.348e-02     &    67.288  &        0.000         &     [  0.880,  0.933]       \\
\bottomrule
\end{tabular}
%\caption{Constant Mean - GJR-GARCH Model Results}
\end{center}

  
\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:} &         RUT        & \textbf{  R-squared:         } &     0.000   \\
\textbf{Mean Model:}    &   Constant Mean    & \textbf{  Adj. R-squared:    } &     0.000   \\
\textbf{Vol Model:}     &       EGARCH       & \textbf{  Log-Likelihood:    } &   -6690.03  \\
\textbf{Distribution:}  &       Normal       & \textbf{  AIC:               } &    13390.1  \\
\textbf{Method:}        & Maximum Likelihood & \textbf{  BIC:               } &    13421.5  \\
\textbf{}               &                    & \textbf{  No. Observations:  } &    3949     \\
\textbf{Date:}          &  Mon, Jun 06 2022  & \textbf{  Df Residuals:      } &    3948     \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{mu} &      -0.0197  &    1.894e-02     &    -1.039  &          0.299       &   [-5.681e-02,1.743e-02]    \\
                  & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{omega}    &       0.0135  &    4.078e-03     &     3.313  &      9.231e-04       &   [5.518e-03,2.150e-02]     \\
\textbf{alpha[1]} &       0.1247  &    1.421e-02     &     8.772  &      1.757e-18       &    [9.681e-02,  0.153]      \\
\textbf{gamma[1]} &      -0.1104  &    1.237e-02     &    -8.921  &      4.601e-19       &    [ -0.135,-8.612e-02]     \\
\textbf{beta[1]}  &       0.9779  &    4.507e-03     &   216.972  &        0.000         &     [  0.969,  0.987]       \\
\bottomrule
\end{tabular}
%\caption{Constant Mean - EGARCH Model Results}
\end{center}
}
\end{multicols}
\caption{GSPC, DJI, IXIC, RUT Arch Models}
\label{first_8}
\end{figure}


\begin{figure}

\begin{multicols}{2}
{\fontsize{2.5}{4}\selectfont
  
\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:} &        SSMI        & \textbf{  R-squared:         } &     0.000   \\
\textbf{Mean Model:}    &   Constant Mean    & \textbf{  Adj. R-squared:    } &     0.000   \\
\textbf{Vol Model:}     &       GARCH        & \textbf{  Log-Likelihood:    } &   -5536.80  \\
\textbf{Distribution:}  &       Normal       & \textbf{  AIC:               } &    11081.6  \\
\textbf{Method:}        & Maximum Likelihood & \textbf{  BIC:               } &    11106.7  \\
\textbf{}               &                    & \textbf{  No. Observations:  } &    3940     \\
\textbf{Date:}          &  Mon, Jun 06 2022  & \textbf{  Df Residuals:      } &    3939     \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{mu} &       0.0415  &    1.368e-02     &     3.032  &      2.429e-03       &   [1.466e-02,6.828e-02]     \\
                  & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{omega}    &       0.0283  &    6.045e-03     &     4.681  &      2.861e-06       &   [1.645e-02,4.014e-02]     \\
\textbf{alpha[1]} &       0.1276  &    1.506e-02     &     8.476  &      2.326e-17       &    [9.812e-02,  0.157]      \\
\textbf{beta[1]}  &       0.8534  &    1.484e-02     &    57.503  &        0.000         &     [  0.824,  0.882]       \\
\bottomrule
\end{tabular}
%\caption{Constant Mean - GARCH Model Results}
\end{center}

\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:} &        SSMI        & \textbf{  R-squared:         } &     0.000   \\
\textbf{Mean Model:}    &   Constant Mean    & \textbf{  Adj. R-squared:    } &     0.000   \\
\textbf{Vol Model:}     &     GJR-GARCH      & \textbf{  Log-Likelihood:    } &   -5467.91  \\
\textbf{Distribution:}  &       Normal       & \textbf{  AIC:               } &    10945.8  \\
\textbf{Method:}        & Maximum Likelihood & \textbf{  BIC:               } &    10977.2  \\
\textbf{}               &                    & \textbf{  No. Observations:  } &    3940     \\
\textbf{Date:}          &  Mon, Jun 06 2022  & \textbf{  Df Residuals:      } &    3939     \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{mu} &   5.3048e-03  &    1.318e-02     &     0.402  &          0.687       &   [-2.053e-02,3.114e-02]    \\
                  & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{omega}    &       0.0269  &    5.124e-03     &     5.248  &      1.537e-07       &   [1.685e-02,3.694e-02]     \\
\textbf{alpha[1]} &       0.0155  &    1.846e-02     &     0.840  &          0.401       &   [-2.068e-02,5.168e-02]    \\
\textbf{gamma[1]} &       0.1768  &    2.422e-02     &     7.299  &      2.906e-13       &     [  0.129,  0.224]       \\
\textbf{beta[1]}  &       0.8749  &    1.496e-02     &    58.500  &        0.000         &     [  0.846,  0.904]       \\
\bottomrule
\end{tabular}
%\caption{Constant Mean - GJR-GARCH Model Results}
\end{center}

  
\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:} &        SSMI        & \textbf{  R-squared:         } &     0.000   \\
\textbf{Mean Model:}    &   Constant Mean    & \textbf{  Adj. R-squared:    } &     0.000   \\
\textbf{Vol Model:}     &       EGARCH       & \textbf{  Log-Likelihood:    } &   -5446.28  \\
\textbf{Distribution:}  &       Normal       & \textbf{  AIC:               } &    10902.6  \\
\textbf{Method:}        & Maximum Likelihood & \textbf{  BIC:               } &    10934.0  \\
\textbf{}               &                    & \textbf{  No. Observations:  } &    3940     \\
\textbf{Date:}          &  Mon, Jun 06 2022  & \textbf{  Df Residuals:      } &    3939     \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{mu} &  -7.9094e-04  &    3.514e-03     &    -0.225  &          0.822       &   [-7.679e-03,6.097e-03]    \\
                  & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{omega}    &   1.6752e-03  &    3.175e-03     &     0.528  &          0.598       &   [-4.549e-03,7.899e-03]    \\
\textbf{alpha[1]} &       0.1501  &    2.216e-02     &     6.774  &      1.249e-11       &     [  0.107,  0.194]       \\
\textbf{gamma[1]} &      -0.1388  &    1.256e-02     &   -11.052  &      2.148e-28       &     [ -0.163, -0.114]       \\
\textbf{beta[1]}  &       0.9733  &    3.940e-03     &   247.002  &        0.000         &     [  0.966,  0.981]       \\
\bottomrule
\end{tabular}
%\caption{Constant Mean - EGARCH Model Results}
\end{center}

  
\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:} &         OEX        & \textbf{  R-squared:         } &     0.000   \\
\textbf{Mean Model:}    &   Constant Mean    & \textbf{  Adj. R-squared:    } &     0.000   \\
\textbf{Vol Model:}     &       GARCH        & \textbf{  Log-Likelihood:    } &   -5607.86  \\
\textbf{Distribution:}  &       Normal       & \textbf{  AIC:               } &    11223.7  \\
\textbf{Method:}        & Maximum Likelihood & \textbf{  BIC:               } &    11248.8  \\
\textbf{}               &                    & \textbf{  No. Observations:  } &    3949     \\
\textbf{Date:}          &  Mon, Jun 06 2022  & \textbf{  Df Residuals:      } &    3948     \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{mu} &       0.0391  &    1.332e-02     &     2.931  &      3.382e-03       &   [1.294e-02,6.517e-02]     \\
                  & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{omega}    &       0.0171  &    4.802e-03     &     3.564  &      3.658e-04       &   [7.702e-03,2.653e-02]     \\
\textbf{alpha[1]} &       0.1007  &    1.303e-02     &     7.732  &      1.060e-14       &    [7.518e-02,  0.126]      \\
\textbf{beta[1]}  &       0.8878  &    1.339e-02     &    66.323  &        0.000         &     [  0.862,  0.914]       \\
\bottomrule
\end{tabular}
%\caption{Constant Mean - GARCH Model Results}
\end{center}

  
\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:} &         OEX        & \textbf{  R-squared:         } &     0.000   \\
\textbf{Mean Model:}    &   Constant Mean    & \textbf{  Adj. R-squared:    } &     0.000   \\
\textbf{Vol Model:}     &     GJR-GARCH      & \textbf{  Log-Likelihood:    } &   -5511.65  \\
\textbf{Distribution:}  &       Normal       & \textbf{  AIC:               } &    11033.3  \\
\textbf{Method:}        & Maximum Likelihood & \textbf{  BIC:               } &    11064.7  \\
\textbf{}               &                    & \textbf{  No. Observations:  } &    3949     \\
\textbf{Date:}          &  Mon, Jun 06 2022  & \textbf{  Df Residuals:      } &    3948     \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{mu} &  -2.0264e-03  &    1.350e-02     &    -0.150  &          0.881       &   [-2.848e-02,2.443e-02]    \\
                  & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{omega}    &       0.0181  &    4.150e-03     &     4.357  &      1.321e-05       &   [9.947e-03,2.622e-02]     \\
\textbf{alpha[1]} &     0.0000    &    1.243e-02     &   0.000    &          1.000       &   [-2.437e-02,2.437e-02]    \\
\textbf{gamma[1]} &       0.1775  &    2.285e-02     &     7.768  &      8.003e-15       &     [  0.133,  0.222]       \\
\textbf{beta[1]}  &       0.8974  &    1.673e-02     &    53.643  &        0.000         &     [  0.865,  0.930]       \\
\bottomrule
\end{tabular}
%\caption{Constant Mean - GJR-GARCH Model Results}
\end{center}

  
\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:} &         OEX        & \textbf{  R-squared:         } &     0.000   \\
\textbf{Mean Model:}    &   Constant Mean    & \textbf{  Adj. R-squared:    } &     0.000   \\
\textbf{Vol Model:}     &       EGARCH       & \textbf{  Log-Likelihood:    } &   -5504.57  \\
\textbf{Distribution:}  &       Normal       & \textbf{  AIC:               } &    11019.1  \\
\textbf{Method:}        & Maximum Likelihood & \textbf{  BIC:               } &    11050.6  \\
\textbf{}               &                    & \textbf{  No. Observations:  } &    3949     \\
\textbf{Date:}          &  Mon, Jun 06 2022  & \textbf{  Df Residuals:      } &    3948     \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{mu} &  -1.5507e-03  &    1.566e-02     & -9.903e-02 &          0.921       &   [-3.224e-02,2.914e-02]    \\
                  & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{omega}    &   1.7588e-03  &    3.350e-03     &     0.525  &          0.600       &   [-4.807e-03,8.325e-03]    \\
\textbf{alpha[1]} &       0.1236  &    1.740e-02     &     7.107  &      1.187e-12       &    [8.954e-02,  0.158]      \\
\textbf{gamma[1]} &      -0.1466  &    1.545e-02     &    -9.489  &      2.329e-21       &     [ -0.177, -0.116]       \\
\textbf{beta[1]}  &       0.9790  &    4.326e-03     &   226.317  &        0.000         &     [  0.970,  0.987]       \\
\bottomrule
\end{tabular}
%\caption{Constant Mean - EGARCH Model Results}
\end{center}

  
\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:} &        N225        & \textbf{  R-squared:         } &     0.000   \\
\textbf{Mean Model:}    &   Constant Mean    & \textbf{  Adj. R-squared:    } &     0.000   \\
\textbf{Vol Model:}     &       GARCH        & \textbf{  Log-Likelihood:    } &   -6639.37  \\
\textbf{Distribution:}  &       Normal       & \textbf{  AIC:               } &    13286.7  \\
\textbf{Method:}        & Maximum Likelihood & \textbf{  BIC:               } &    13311.7  \\
\textbf{}               &                    & \textbf{  No. Observations:  } &    3844     \\
\textbf{Date:}          &  Mon, Jun 06 2022  & \textbf{  Df Residuals:      } &    3843     \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{mu} &       0.0448  &    1.993e-02     &     2.245  &      2.475e-02       &   [5.687e-03,8.382e-02]     \\
                  & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{omega}    &       0.0430  &    1.113e-02     &     3.862  &      1.124e-04       &   [2.117e-02,6.481e-02]     \\
\textbf{alpha[1]} &       0.1056  &    1.407e-02     &     7.506  &      6.091e-14       &    [7.803e-02,  0.133]      \\
\textbf{beta[1]}  &       0.8785  &    1.417e-02     &    62.002  &        0.000         &     [  0.851,  0.906]       \\
\bottomrule
\end{tabular}
%\caption{Constant Mean - GARCH Model Results}
\end{center}

  
\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:} &        N225        & \textbf{  R-squared:         } &     0.000   \\
\textbf{Mean Model:}    &   Constant Mean    & \textbf{  Adj. R-squared:    } &     0.000   \\
\textbf{Vol Model:}     &     GJR-GARCH      & \textbf{  Log-Likelihood:    } &   -6609.71  \\
\textbf{Distribution:}  &       Normal       & \textbf{  AIC:               } &    13229.4  \\
\textbf{Method:}        & Maximum Likelihood & \textbf{  BIC:               } &    13260.7  \\
\textbf{}               &                    & \textbf{  No. Observations:  } &    3844     \\
\textbf{Date:}          &  Mon, Jun 06 2022  & \textbf{  Df Residuals:      } &    3843     \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{mu} &       0.0120  &    1.978e-02     &     0.607  &          0.544       &   [-2.677e-02,5.078e-02]    \\
                  & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{omega}    &       0.0525  &    1.242e-02     &     4.228  &      2.359e-05       &   [2.817e-02,7.687e-02]     \\
\textbf{alpha[1]} &       0.0462  &    1.362e-02     &     3.393  &      6.913e-04       &   [1.951e-02,7.289e-02]     \\
\textbf{gamma[1]} &       0.1075  &    2.699e-02     &     3.983  &      6.813e-05       &    [5.459e-02,  0.160]      \\
\textbf{beta[1]}  &       0.8771  &    1.308e-02     &    67.057  &        0.000         &     [  0.851,  0.903]       \\
\bottomrule
\end{tabular}
%\caption{Constant Mean - GJR-GARCH Model Results}
\end{center}

  
\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:} &        N225        & \textbf{  R-squared:         } &     0.000   \\
\textbf{Mean Model:}    &   Constant Mean    & \textbf{  Adj. R-squared:    } &     0.000   \\
\textbf{Vol Model:}     &       EGARCH       & \textbf{  Log-Likelihood:    } &   -6602.25  \\
\textbf{Distribution:}  &       Normal       & \textbf{  AIC:               } &    13214.5  \\
\textbf{Method:}        & Maximum Likelihood & \textbf{  BIC:               } &    13245.8  \\
\textbf{}               &                    & \textbf{  No. Observations:  } &    3844     \\
\textbf{Date:}          &  Mon, Jun 06 2022  & \textbf{  Df Residuals:      } &    3843     \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{mu} &   2.9055e-03  &    4.866e-03     &     0.597  &          0.550       &   [-6.632e-03,1.244e-02]    \\
                  & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{omega}    &       0.0248  &    5.322e-03     &     4.663  &      3.111e-06       &   [1.439e-02,3.525e-02]     \\
\textbf{alpha[1]} &       0.1982  &    2.023e-02     &     9.800  &      1.130e-22       &     [  0.159,  0.238]       \\
\textbf{gamma[1]} &      -0.0882  &    1.833e-02     &    -4.810  &      1.509e-06       &    [ -0.124,-5.224e-02]     \\
\textbf{beta[1]}  &       0.9661  &    6.458e-03     &   149.601  &        0.000         &     [  0.953,  0.979]       \\
\bottomrule
\end{tabular}
%\caption{Constant Mean - EGARCH Model Results}
\end{center}

  
\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:} &        FTSE        & \textbf{  R-squared:         } &     0.000   \\
\textbf{Mean Model:}    &   Constant Mean    & \textbf{  Adj. R-squared:    } &     0.000   \\
\textbf{Vol Model:}     &       GARCH        & \textbf{  Log-Likelihood:    } &   -5610.32  \\
\textbf{Distribution:}  &       Normal       & \textbf{  AIC:               } &    11228.6  \\
\textbf{Method:}        & Maximum Likelihood & \textbf{  BIC:               } &    11253.8  \\
\textbf{}               &                    & \textbf{  No. Observations:  } &    3964     \\
\textbf{Date:}          &  Mon, Jun 06 2022  & \textbf{  Df Residuals:      } &    3963     \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{mu} &       0.0339  &    1.343e-02     &     2.526  &      1.153e-02       &   [7.605e-03,6.024e-02]     \\
                  & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{omega}    &       0.0158  &    4.456e-03     &     3.542  &      3.973e-04       &   [7.049e-03,2.452e-02]     \\
\textbf{alpha[1]} &       0.1101  &    1.554e-02     &     7.082  &      1.418e-12       &    [7.962e-02,  0.141]      \\
\textbf{beta[1]}  &       0.8809  &    1.612e-02     &    54.662  &        0.000         &     [  0.849,  0.912]       \\
\bottomrule
\end{tabular}
%\caption{Constant Mean - GARCH Model Results}
\end{center}

  
\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:} &        FTSE        & \textbf{  R-squared:         } &     0.000   \\
\textbf{Mean Model:}    &   Constant Mean    & \textbf{  Adj. R-squared:    } &     0.000   \\
\textbf{Vol Model:}     &     GJR-GARCH      & \textbf{  Log-Likelihood:    } &   -5521.98  \\
\textbf{Distribution:}  &       Normal       & \textbf{  AIC:               } &    11054.0  \\
\textbf{Method:}        & Maximum Likelihood & \textbf{  BIC:               } &    11085.4  \\
\textbf{}               &                    & \textbf{  No. Observations:  } &    3964     \\
\textbf{Date:}          &  Mon, Jun 06 2022  & \textbf{  Df Residuals:      } &    3963     \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{mu} &  -8.3235e-03  &    1.331e-02     &    -0.625  &          0.532       &   [-3.442e-02,1.777e-02]    \\
                  & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{omega}    &       0.0185  &    4.211e-03     &     4.387  &      1.150e-05       &   [1.022e-02,2.673e-02]     \\
\textbf{alpha[1]} &   5.5212e-10  &    9.449e-03     & 5.843e-08  &          1.000       &   [-1.852e-02,1.852e-02]    \\
\textbf{gamma[1]} &       0.1718  &    2.367e-02     &     7.256  &      3.992e-13       &     [  0.125,  0.218]       \\
\textbf{beta[1]}  &       0.8994  &    1.505e-02     &    59.746  &        0.000         &     [  0.870,  0.929]       \\
\bottomrule
\end{tabular}
%\caption{Constant Mean - GJR-GARCH Model Results}
\end{center}

  
\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:} &        FTSE        & \textbf{  R-squared:         } &     0.000   \\
\textbf{Mean Model:}    &   Constant Mean    & \textbf{  Adj. R-squared:    } &     0.000   \\
\textbf{Vol Model:}     &       EGARCH       & \textbf{  Log-Likelihood:    } &   -5517.81  \\
\textbf{Distribution:}  &       Normal       & \textbf{  AIC:               } &    11045.6  \\
\textbf{Method:}        & Maximum Likelihood & \textbf{  BIC:               } &    11077.0  \\
\textbf{}               &                    & \textbf{  No. Observations:  } &    3964     \\
\textbf{Date:}          &  Mon, Jun 06 2022  & \textbf{  Df Residuals:      } &    3963     \\
\bottomrule
\end{tabular}
\begin{tabular}{lccccc}
            & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{mu} &      -0.0103  &    1.346e-02     &    -0.767  &          0.443       &   [-3.671e-02,1.606e-02]    \\
                  & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{95.0\% Conf. Int.}  \\
\midrule
\textbf{omega}    &   1.0930e-03  &    2.720e-03     &     0.402  &          0.688       &   [-4.237e-03,6.423e-03]    \\
\textbf{alpha[1]} &       0.1227  &    1.770e-02     &     6.931  &      4.183e-12       &    [8.799e-02,  0.157]      \\
\textbf{gamma[1]} &      -0.1274  &    1.287e-02     &    -9.896  &      4.346e-23       &     [ -0.153, -0.102]       \\
\textbf{beta[1]}  &       0.9824  &    3.473e-03     &   282.844  &        0.000         &     [  0.976,  0.989]       \\
\bottomrule
\end{tabular}
%\caption{Constant Mean - EGARCH Model Results}
\end{center}


}
\end{multicols}
\caption{SSMI, OEX, N225, FTSE Arch Models}
\label{second_8}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width = 12.4cm]{figures/VOLA_FIGURS.png}
    \caption{Conditional Volatilities on the Test Set and Realized Volatility}
    \label{fig:my_label}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width = 13.4cm]{figures/QQ_plot.png}
    \caption{ Q-Q Plots of the Residuals}
    \label{fig:my_label}
\end{figure}
\subsection{Multi-Dimensional Data}\label{Multi_data}

Figures \ref{HB_P}, \ref{FTSE_P} and \ref{SMI_P} illustrate the historical prices of every portfolio of the multivariate analysis together with the Linear Correlation Matrix of their percentage returns.

\begin{figure}[H]
    \centering
    \includegraphics[width = 13cm]{figures/HB_data.png}
    \caption{BH Portfolio}
    \label{HB_P}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width = 13cm]{figures/FTSE_data.png}
    \caption{FTSE Portfolio}
    \label{FTSE_P}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width = 13cm]{figures/smi_data.png}
    \caption{SMI Portfolio}
    \label{SMI_P}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width = 10cm]{figures/HB_vola.png}
    \caption{Volatility Comparison | HB Portfolio}
    \label{fig:my_label}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width = 10cm]{figures/FTSE_vola.png}
    \caption{Volatility Comparison | FTSE Portfolio}
    \label{fig:my_label}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width = 10cm]{figures/SMI_vola.png}
    \caption{Volatility Comparison | SMI Portfolio}
    \label{fig:my_label}
\end{figure}

\section{Additional Figures}

\subsection{Loss Surface of the DCC model}
Figure \ref{dcc_surf} illustrates the loss function surface of the DCC-GARCH model for the SMI portfolio. The convexity of the surface is clearly visualized as well as the non well-posed issues that appear when the constraint that $\alpha+\beta<1$ is violated.

\begin{figure}[H]
    \centering
    \includegraphics[width = 12cm]{figures/dcc_loss.png}
    \caption{Loss Surface of the DCC Model}
    \label{dcc_surf}
\end{figure}

\subsection{Additional Figures from Chapter \ref{Lombard}}
\begin{figure}[H]
    \centering
    \includegraphics[width = 13cm]{figures/ll_portf.png}
    \caption{Lombard Porfolio}
    \label{ll_portf}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width = 13cm]{figures/kde.png}
    \caption{KDE of the Simulations}
    \label{fig:kde}
\end{figure}
%---------------------------------------------------------------

\printbibliography

\end{document}